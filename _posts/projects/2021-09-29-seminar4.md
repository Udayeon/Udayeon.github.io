---
layout: post
title: Seminar4 
description: |
tags:
  - projects
use_math : true
comments : true
author: Udayeon

published: true
---

논문 작성을 위한 주제잡기 과정

# 목차
1. 자율주행 기술동향
2. 연구방향
3. 실험
4. 결과
5. 고찰


# 1. 자율주행차량 기술동향
* * *
자율주행 기술이란 차량이 스스로 주변 환경을 인지하고 상황에 맞는 판단에 따라 적절히 제어하여 목적지까지 안전하게 도달하는 기술을 말한다. 사람의 눈은 매우 직관적이라 인지하고 판단을 내리는 데에
아주 짧은 시간만이 필요하지만 기계는 그렇지 않다. 대상의 형태,색,명도 등 다양한 시각적 정보를 알고리즘에 따라 처리한 후에야 비로소 사물의 정체를 알게 된다. 따라서 자율주행 차량이 사람의 눈처럼 
빠르고 정확하게 객체를 인식하는 것이 자율주행기술의 과업이라 할 수 있겠다. 이를 위해, 사람의 눈과 같은 기능을 해주는 다양한 센서들을 차량에 부착한다. 센서의 예로는 카메라, 라이다, 레이더가 있다.
각각의 센서는 장단점이 다르므로 죄근 센서퓨전이라 하는 기술을 통해 이들을 다양하게 조합해 사용하기도 한다.   
앞서, 자율주행기술을 크게 인지,판단,제어 세 가지로 나눈단 것을 언급했다. 가장 먼저 '인지(Perception)'에 대해 다루고자 한다. 설명한대로 기계의 인지 능력은 사람의 눈처럼 직관적이지 않으므로
사물을 인식하기 위해선 상대적으로 오랜 시간이 필요하고 그 정확도도 떨어지는 실정이다. 이런 문제점을 보완하는 것이 인지 기술의 핵심이라 할 수 있겠다. 객체 인식을 위한 다양한 알고리즘이 존재하겠지만
단연 독보적으로 정확한 알고리즘은 딥러닝으로 훈련시킨 알고리즘이다. 딥러닝이란 인간이 뇌에서 신경세포를 시용하는 것과 유사한 방식으로 알고리즘을 
활용하는 기술을 말한다. 예시 데이터를 통해 컴퓨터가 학습하고 새로운 데이터가 들어와도 이에 대처할 수 있는 능력을 갖게 되는 것이다.
딥러닝 기반의 다양한 객체인식 알고리즘이 있고 이를 구현하고자 한다.


# 2.Sensor Fusion
* * *
Matlab sensor fusion 구현하기

## 2.1. Extended Object Tracking of Highway Vehicles with Radar and Camera
* * *
### (1) 개요
이 예제는 ego vehicle이 고속도로에서 주변차량의 어떻게 추적하는지를 보여준다. 차량은 확장된 객체로, 그 크기가 다수의 resolution cell에
걸쳐있게 된다. 결과적으로, 센서는 한 번의 스캔으로 이 객체의 검출을 보고한다. 이 예제에서, 다른 extendee object tracking 기술을 이용해
고속도로의 차량을 추적하고 tracking성능을 검증할 수 있다.
   
기존의 tracking 접근법(multiObjectTracker/trackerGNN, trakcerJPDA/multi-hypothesis tracking)을 통해 추적된 객체는 센서 스캔 당
하나의 detection을 반환하는 것으로 가정한다. 고해상도 radar와 같이 더 나은 resolution을 갖는 센서의 개발로, 센서들은 일반적으로
객체에 대한 한 번의 detection보다 더 많이 반환하게 되었다. 예를 들어, 아래의 이미지는 Radar의 여러 resolution cell에 걸쳐있는 한 차량에
대한 multiple detection을 보여준다. 이런 경우, 객체 추적에 사용된 기술은 **extended object tracking**으로 알려져있다.
![image](https://user-images.githubusercontent.com/69246778/137067319-ac2bb3d5-6ef0-43ae-a851-33781765e69f.png)
<고해상도 센서를 이용한 스캔 한번으로 객체에 대한 여러 detection을 return함>
   
고해상도 센서 이용의 가장 중요한 이점은 객체의 치수 및 방향과 같은 물체에 대한 더 많은 정보를 얻을 수 있다는 것이다. 이 추가 정보들은
탐지 확률을 높이고 오탐확률을 줄여줄 수 있다.   
   
확장된 객체는 기존의 tracker에 새로운 과업을 제시한다. 왜냐하면 기존의 tracker들은 센서하나당 객체하나, 그리고 객체 하나당 한 번의 탐지를
가정하기 때문이다. 몇몇 경우에, 객체 하나 당 단일 탐지를 실행하는 기존 tracker를 위해 센서 데이터를 cluster할 수 있다. 그러나, 그렇게
하게되면, 고해상도 센서를 사용하는 이유가 없다.   
   
반면, extended object tracker는 객체 당 여러 탐지를 다룬다. 게다가, 이 tracker는 위치나 속도 같은 운동 상태뿐 아니라 물체의 차원과
방향도 추정할 수 있다. 예를 들어, 다음의 tracker들을 이용해 ego vehicle 주변 차량을 추적한다고 하자.
* 기존의 tracker : multi object tracker ( point target model이용)
* GGIW-PHD tracker : Gamma Gaussian Inverse Wishart PHD, GGIW필터를 사용한 PHD tracker
* GM-PHD tracker : Gaussian Mixture PHD, (rectangular filter 이용)

tracker의 성능 평가는 **trackErrorMetrics**와 **trackAssignmentMetrics**를 사용한다. 이는 tracker의 모든 효율 측정 결과를 평가한다. 
**trackOSPAMetric(Optimal SubPattern Assignment Metric)** 을 이용해 점수로써 tracker의 성능을 평가한다.

### (2) 시나리오 만들기

```
% Create the scenario
exPath = fullfile(matlabroot,'examples','driving_fusion','main'); %파일에 대한 전체 경로를 포함한 문자형 벡터 반환
                                                                  %파일 경로 : examples\driving_fusion\main
addpath(exPath)
[scenario, egoVehicle, sensors] = helperCreateScenario; 

% Create the display object
display = helperExtendedTargetTrackingDisplay;

% Create the Animation writer to record each frame of the figure for
% animation writing. Set 'RecordGIF' to true to enable GIF writing.
gifWriter = helperGIFWriter('Figure',display.Figure,...
    'RecordGIF',false);
```

### (2) Metrics 만들기(성능계산)
```
% Function to return the errors given track and truth.
errorFcn = @(track,truth)helperExtendedTargetError(track,truth);

% Function to return the distance between track and truth.
distFcn = @(track,truth)helperExtendedTargetDistance(track,truth);

% Function to return the IDs from the ground truth. The default
% identifier assumes that the truth is identified with PlatformID. In
% drivingScenario, truth is identified with an ActorID.
truthIdFcn = @(x)[x.ActorID];

% Create metrics object.
tem = trackErrorMetrics(...
    'ErrorFunctionFormat','custom',...
    'EstimationErrorLabels',{'PositionError','VelocityError','DimensionsError','YawError'},...
    'EstimationErrorFcn',errorFcn,...
    'TruthIdentifierFcn',truthIdFcn);

tam = trackAssignmentMetrics(...
    'DistanceFunctionFormat','custom',...
    'AssignmentDistanceFcn',distFcn,...
    'DivergenceDistanceFcn',distFcn,...
    'TruthIdentifierFcn',truthIdFcn,...
    'AssignmentThreshold',30,...
    'DivergenceThreshold',35);

% Create ospa metric object.
tom = trackOSPAMetric(...
    'Distance','custom',...
    'DistanceFcn',distFcn,...
    'TruthIdentifierFcn',truthIdFcn);
```

### (3)-1. Point Obejct Tracker
```
trackerRunTimes = zeros(0,3);
ospaMetric = zeros(0,3);

% Create a multiObjectTracker
tracker = multiObjectTracker(...
    'FilterInitializationFcn', @helperInitPointFilter, ...
    'AssignmentThreshold', 30, ...
    'ConfirmationThreshold', [4 5], ...
    'DeletionThreshold', 3);

% Reset the random number generator for repeatable results
seed = 2018;
S = rng(seed);
timeStep = 1;

% For multiObjectTracker, the radar reports in Ego Cartesian frame and does
% not report velocity. This allows us to cluster detections from multiple
% sensors.
for i = 1:6
    sensors{i}.HasRangeRate = false;
    sensors{i}.DetectionCoordinates = 'Body';
end
```
```
while advance(scenario) && ishghandle(display.Figure)
    % Get the scenario time
    time = scenario.SimulationTime;

    % Collect detections from the ego vehicle sensors
    [detections,isValidTime] = helperDetect(sensors, egoVehicle, time);

    % Update the tracker if there are new detections
    if any(isValidTime)
        % Detections must be clustered first for the point tracker
        detectionClusters = helperClusterRadarDetections(detections);

        % Update the tracker
        tic
        % confirmedTracks are in scenario coordinates
        confirmedTracks = updateTracks(tracker, detectionClusters, time);
        t = toc;

        % Update the metrics
        % a. Obtain ground truth
        groundTruth = scenario.Actors(2:end); % All except Ego

        % b. Update assignment metrics
        tam(confirmedTracks,groundTruth);
        [trackIDs,truthIDs] = currentAssignment(tam);

        % c. Update error metrics
        tem(confirmedTracks,trackIDs,groundTruth,truthIDs);

        % d. Update ospa metric
        ospaMetric(timeStep,1) = tom(confirmedTracks, groundTruth);

        % Update bird's-eye-plot
        % Convert tracks to ego coordinates for display
        confirmedTracksEgo = helperConvertToEgoCoordinates(egoVehicle, confirmedTracks);
        display(egoVehicle, sensors, detections, confirmedTracksEgo, detectionClusters);
        drawnow;

        % Record tracker run times
        trackerRunTimes(timeStep,1) = t;
        timeStep = timeStep + 1;

        % Capture frames for animation
        gifWriter();
    end
end

% Capture the cumulative track metrics. The error metrics show the averaged
% value of the error over the simulation.
assignmentMetricsMOT = tam.trackMetricsTable;
errorMetricsMOT = tem.cumulativeTruthMetrics;

% Write GIF if requested
writeAnimation(gifWriter,'multiObjectTracking');
```
### (3)-2. GGIW-PHD Extended Object Tracker
```
% Set up sensor configurations
%
sensorConfigurations = helperCreateSensorConfigurations(sensors,egoVehicle);

% The transform function and filter initialization functions are state and
% filter dependent. Therefore, they are not set in the helper function.
for i = 1:numel(sensorConfigurations)
    % You can use a different technique to initialize a filter for each
    % sensor by using a different function for each configuration.
    sensorConfigurations{i}.FilterInitializationFcn = @helperInitGGIWFilter;

    % Tracks are defined in constant turn-rate state-space in the scenario
    % coordinates. The MeasurementFcn for constant turn-rate model can be
    % used as the transform function.
    sensorConfigurations{i}.SensorTransformFcn = @ctmeas;
end
```
```
partFcn = @(x)helperPartitioningFcn(x,3,5);

tracker = trackerPHD('SensorConfigurations', sensorConfigurations,...
    'PartitioningFcn',partFcn,...
    'AssignmentThreshold',450,...% Minimum negative log-likelihood of a detection cell (multiple detections per cell) to add birth components.
    'ExtractionThreshold',0.75,...% Weight threshold of a filter component to be declared a track
    'ConfirmationThreshold',0.85,...% Weight threshold of a filter component to be declared a confirmed track
    'MergingThreshold',50,...% Threshold to merge components
    'HasSensorConfigurationsInput',true... % Tracking is performed in scenario frame and hence sensor configurations change with time
    );
```
```
% Release and restart all objects.
restart(scenario);
release(tem);
release(tam);
% No penality for trackerPHD
tam.AssignmentThreshold = tam.AssignmentThreshold - 2;
release(display);
display.PlotClusteredDetection = false;
gifWriter.pFrames = {};
for i = 1:numel(sensors)
    release(sensors{i});
    if i <= 6
        sensors{i}.HasRangeRate = true;
        sensors{i}.DetectionCoordinates = 'Sensor spherical';
    end
end

% Restore random seed.
rng(seed)

% First time step
timeStep = 1;
% Run the scenario
while advance(scenario) && ishghandle(display.Figure)
    % Get the scenario time
    time = scenario.SimulationTime;

    % Get the poses of the other vehicles in ego vehicle coordinates
    ta = targetPoses(egoVehicle);

    % Collect detections from the ego vehicle sensors
    [detections, isValidTime, configurations] = helperDetect(sensors, egoVehicle, time, sensorConfigurations);

    % Update the tracker with all the detections. Note that there is no
    % need to cluster the detections before passing them to the tracker.
    % Also, the sensor configurations are passed as an input to the
    % tracker.
    tic
    % confirmedTracks are in scenario coordinates
    confirmedTracks = tracker(detections,configurations,time);
    t = toc;

    % Update the metrics
    % a. Obtain ground truth
    groundTruth = scenario.Actors(2:end); % All except Ego

    % b. Update assignment metrics
    tam(confirmedTracks,groundTruth);
    [trackIDs,truthIDs] = currentAssignment(tam);

    % c. Update error metrics
    tem(confirmedTracks,trackIDs,groundTruth,truthIDs);

    % d. Update ospa metric
    ospaMetric(timeStep,2) = tom(confirmedTracks, groundTruth);

    % Update the bird's-eye plot
    % Convert tracks to ego coordinates for display
    confirmedTracksEgo = helperConvertToEgoCoordinates(egoVehicle, confirmedTracks);
    display(egoVehicle, sensors, detections, confirmedTracksEgo);
    drawnow;

    % Record tracker run times
    trackerRunTimes(timeStep,2) = t;
    timeStep = timeStep + 1;

    % Capture frames for GIF
    gifWriter();
end

% Capture the truth and track metrics tables
assignmentMetricsGGIWPHD = tam.trackMetricsTable;
errorMetricsGGIWPHD = tem.cumulativeTruthMetrics;

% Write GIF if requested
writeAnimation(gifWriter,'ggiwphdTracking');
```


### (3)-3. GM-PHD Rectangular Object Tracker
```
for i = 1:numel(sensorConfigurations)
    sensorConfigurations{i}.FilterInitializationFcn = @helperInitRectangularFilter; % Initialize a rectangular target gmphd
    sensorConfigurations{i}.SensorTransformFcn = @ctrectcorners; % Use corners to calculate detection probability
end

% Define tracker using new sensor configurations
tracker = trackerPHD('SensorConfigurations', sensorConfigurations,...
    'PartitioningFcn',partFcn,...
    'AssignmentThreshold',600,...% Minimum negative log-likelihood of a detection cell to add birth components
    'ExtractionThreshold',0.85,...% Weight threshold of a filter component to be declared a track
    'ConfirmationThreshold',0.95,...% Weight threshold of a filter component to be declared a confirmed track
    'MergingThreshold',50,...% Threshold to merge components
    'HasSensorConfigurationsInput',true... % Tracking is performed in scenario frame and hence sensor configurations change with time
    );

% Release and restart all objects.
restart(scenario);
for i = 1:numel(sensors)
    release(sensors{i});
end
release(tem);
release(tam);
release(display);
display.PlotClusteredDetection = false;
gifWriter.pFrames = {};

% Restore random seed.
rng(seed)

% First time step
timeStep = 1;

% Run the scenario
while advance(scenario) && ishghandle(display.Figure)
    % Get the scenario time
    time = scenario.SimulationTime;

    % Get the poses of the other vehicles in ego vehicle coordinates
    ta = targetPoses(egoVehicle);

    % Collect detections from the ego vehicle sensors
    [detections, isValidTime, configurations] = helperDetect(sensors, egoVehicle, time, sensorConfigurations);

    % Update the tracker with all the detections. Note that there is no
    % need to cluster the detections before passing them to the tracker.
    % Also, the sensor configurations are passed as an input to the
    % tracker.
    tic
    % confirmedTracks are in scenario coordinates
    confirmedTracks = tracker(detections,configurations,time);
    t = toc;

    % Update the metrics
    % a. Obtain ground truth
    groundTruth = scenario.Actors(2:end); % All except Ego

    % b. Update assignment metrics
    tam(confirmedTracks,groundTruth);
    [trackIDs,truthIDs] = currentAssignment(tam);

    % c. Update error metrics
    tem(confirmedTracks,trackIDs,groundTruth,truthIDs);

    % d. Update ospa metric
    ospaMetric(timeStep,3) = tom(confirmedTracks, groundTruth);

    % Update the bird's-eye plot
    % Convert tracks to ego coordinates for display
    confirmedTracksEgo = helperConvertToEgoCoordinates(egoVehicle, confirmedTracks);
    display(egoVehicle, sensors, detections, confirmedTracksEgo);
    drawnow;

    % Record tracker run times
    trackerRunTimes(timeStep,3) = t;
    timeStep = timeStep + 1;

    % Capture frames for GIF
    gifWriter();
end

% Capture the truth and track metrics tables
assignmentMetricsGMPHD = tam.trackMetricsTable;
errorMetricsGMPHD = tem.cumulativeTruthMetrics;

% Write GIF if requested
writeAnimation(gifWriter,'gmphdTracking');

% Return the random number generator to its previous state
rng(S)
rmpath(exPath)
```

### (4) 성능평가
```
helperPlotAssignmentMetrics(assignmentMetricsMOT, assignmentMetricsGGIWPHD, assignmentMetricsGMPHD);
helperPlotErrorMetrics(errorMetricsMOT, errorMetricsGGIWPHD, errorMetricsGMPHD);
```




