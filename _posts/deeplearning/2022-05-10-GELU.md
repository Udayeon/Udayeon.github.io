---
layout: post
title: 
description: |
  활성화함수 GELU
hide_image: true
tags:
  - deeplearning
published: false
---

# GELU
* * *
[링크](https://arxiv.org/pdf/1606.08415.pdf)


# 1. GELU(Gaussian Error Linear Unit)란?
BERT(Bidirectional Encoder Representations from Transformers), 
GPT(Generative Pre-trained Transformer), ViT모델에서 사용하는 활성화홤수. 최신 NLP, vision SOTA 성능을
도출하는 모델들이 주로 사용함.

# 2. GELU 형태
Dropout, zoneout, ReLU함수의 특성을 조합하여 유도됨. 

# 3. GELU 메리트?

어쩔저쩔
