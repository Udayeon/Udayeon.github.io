---
layout: post
title: 
description: |
  LSTM(Long Short-Term Memory models,장단기메모리)
hide_image: true
tags:
  - deeplearning
published: True
---

# LSTM(Long Short-Term Memory models)
* * *

# 1. RNN
**R**ecurrent **N**eural **N**etwork, 순환신경망. 입출력을 **Sequence**단위로 처리한다. 기본적인 신경망은 **은닉층→출력층** 방향으로 
향하는 **Feed Forward Neural Network**방식인데 RNN은 그렇지 않다. **은닉층→출력층**으로 보내면서 **다시 은닉층의 input으로 사용**하기도
한다.
![image](https://user-images.githubusercontent.com/69246778/149880121-3820b26d-4573-4a4c-bf94-b0c9f70a7fef.png)
위 그림에서 입력벡터 x_t는 중간에 은닉층을 거쳐 출력벡터인 y_t로 나가야하지만 일부는 다시 은닉층의 일부로 사용된다. 이때, 이 노드는 
이전의 값을 기억하려 하므로 **메모리 셀** 또는 **RNN 셀**이라고 한다.
   
![image](https://user-images.githubusercontent.com/69246778/149888745-61947708-2db6-48ee-ab29-688f4e0385e8.png)
위의 그림에서 첨자는 시각 t를 나타낸다. 즉, t=1일 때의 output중 일부는 y_1로 나가고, 일부는 t=2의 input으로 다시 들어간다. 이 과정이 반복.


# 2. LSTM
![image](https://user-images.githubusercontent.com/69246778/149890336-068c1aa5-574e-4527-9618-bca5d870aef8.png)
time step이 길어질수록 색이 점점 옅어진다.(정보량 손실)   

가장 기본적인 RNN을 **Vanilla(바닐라) RNN**이라고 한다. 바닐라 RNN은 비교적 짧은 시퀀스에만 효과를 보이는 단점이 있다. 바닐라 RNN은
time step이 길어질수록 초기의 정보량이 손실된다. 만약 초기에 중요 정보가 있었다면 이를 손실하게 될 수도 있다. 즉, 바닐라 RNN은 초기 정보를
충분히 오래 기억하지 못하는데 이를 **장기 의존성 문제(the problem of Long-Term Dependencies)** 라 한다. 이러한 문제를 해결하기 위한
방안으로 사용된 것이 **장단기메모리(Long Short-Term Memory,LSTM)** 이다.   
   
LSTM은 은닉층의 메모리 셀에 **입력,망각,출력**gate를 추가해 불필요한



# 3. GRU

# 4. 예제로 알아보는 LSTM
