---
layout: post
title: 
description: |
  Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, 
  Zheng Zhang, Stephen Lin, Baining Guo, 
  Microsoft Research Asia
hide_image: true
tags:
  - review
published: true
---

# Swin Transformer: Hierarchical Vision Transformer using Shifted Windows
[Link](https://arxiv.org/pdf/2103.14030.pdf)
* * *

## Abstract
텍스트와 이미지의 차이점(**스케일변화 및 고해상도**)으로 인해 Transformer를 비전 분야에 적용하는 것이 어렵다. SwinTransformer는
이런 문제를 해결하기 위해 **Shifted Windows를 사용하는 계층적인 Transformer**를 제안한다. 어텐션 영역을 겹치지 않는 윈도우로
제한하여 **효율성을 높이고, 이미지 크기에 대해 선형적인 계산 복잡성을 제공한다. 또한, 다양한 스케일에 대한 모델링 유연성을 
갖고 모든 MLP 아키텍처에 적용가능**하다.   

## Introduction
![image](https://user-images.githubusercontent.com/69246778/222346760-a1ad084e-5294-4eed-b023-1323fa27cc50.png)   
기존 Vision Transformer는 저해상도의 단일 피쳐맵에 대해서 전역적인 어텐션을 수행했다. 반면, Swin은 계층적인 피처맵을 구성하며
각 윈도우 내에서만 어텐션을 수행하므로 이미지 크기에 대해 선형적인 계산 복잡도를 가진다.   
   
   
![image](https://user-images.githubusercontent.com/69246778/222346677-c8d61b2a-dff7-4305-b071-6007ec72ae0c.png)   
Swin Transformer의 핵심 디자인 중 하나는 **Shifted Window**이다. 먼저 일반적인 Window Partioning으로 셀프 어텐션을 수행하고 
다음 레이어에서 partion을 이동시킨 후 새로운 window기반의 셀프 어텐션을 수행한다. 새로운 window는 이전 레이어의 윈도우간 경계를 
포함한다.   
   
   
![image](https://user-images.githubusercontent.com/69246778/222346075-ed7cbd4c-d8a6-4986-a5f9-7b3883d43fad.png)   
![image](https://user-images.githubusercontent.com/69246778/222346300-0c66af74-d872-4582-b3dd-2f3bf1913fe4.png)   
이전의 Sliding Window기반의 어텐션은 쿼리마다 다른 key를 공유하지만 **Shifted Window는 동일한 key를 공유하므로 하드웨어에서 
메모리 엑세스가 용이하다.** 위의 Table5,6에서도 알 수 있듯이 모델링 파워는 비슷하지만 FPS가 훨씬 개선된 것을 확인할 수 있다.   
   
   
## Related Work
* **CNN and variants**   
CNN은 컴퓨터 비전의 대표적인 네트워크 모델로 사용된다. AlexNet, VGG, GoogleNet, ResNet, DenseNet, HRNet, 
EfficientNet등 깊고 효과적인 합성곱 신경망 구조가 제안되었고 그 밖에 Depthwise convolution과 deformable convolution과 같은 개별 합성곱
레이어 개선에 대한 연구도 진행되고 있다. **CNN은 여전히 중요한 비전 네트워크지만 Tranformer같은 구조들이 기본적인 비전 인식 
태스크에서 매우 좋은 성능을 달성했고, 이것이 비전과 언어를 통합 모델링 할 수 있을 것으로 기대한다.**   
   
* **Self-attention based backbone architectures**   
이전 논문들("Local relation networks for image recognition","Stand-alone self attention in vision models",
"Exploring self-attention for image recognition")에서 특정 영역 내에서만 어텐션을 수행하는 방법을 제안하여 ResNet보다 성능을 달성했다. 
그러나 이들은 메모리 엑세스에 비용이 많이 드므로 convolution 네트워크보다 더 큰 지연시간을 발생시킨다. Swin Transformer는 
**Shift Windows**를 사용하여 하드웨어에서 더 효율적인 구현이 가능하도록 한다.
   
* **Self-attention/Transformers to complement CNNs**      
CNN에 self-attention layer나 transformer를 추가하는 연구도 있다. Swin은 기본적인 시각적 피처 추출에 transformer를 적용한다. 
   
* **Transformer based vision backbones**   
우리 연구와 가장 관련 있는 것은 Vision Transformer부분이다. 기존 ViT는 중간 크기 정도의 이미지 패치에 Transformer를 적용한다. 높은 
성능에 달성하는 대신 대규모의 훈련 데이터셋(JFT-300M)이 필요하다. DeiT는 ViT가 적은 데이터셋(ImageNet1K)를 사용하여도 효과적인 성능을 내는
방법 몇 가지를 소개한다. 그러나, ViT는 이미지 해상도가 높은 경우엔 적합하지 않다. 왜냐하면 복잡성이 이미지의 크기에 따라 제곱으로 증가하기 
때문이다. ViT를 개선한 다른 연구들도 여전히 이미지 크기에 따라 제곱적으로 복잡성이 증가한다. **swin은 선형적인 증가를 제안한다.**

## Method

![image](https://user-images.githubusercontent.com/69246778/222354587-694f47e5-1232-4ae3-8418-3127bf0f8ac1.png)   
### Overall Architecture
총 4개의 단계로 구성되어 있다. 각 Stage는 PatchMerging하여 해상도 조정하는 부분과 SwinBlock으로 구성되어 있다. 각 단계별
SwinBlock의 갯수는 [2,2,6,2]개이다.   
   
먼저 input이미지를 4x4크기의 패치로 나눈다. 4x4크기로 나눈 후에 하나의 패치 뒤로 나머지 세 개를 임베딩 시킨다고 생각하면 돼서 
이미지의 크기는 1/4로 줄고, 3채널 이미지 이므로 4x4x3 = 48차원이 된다. 이게 stage1의 input이 된다. stage1의 결과를 stage2로 넣고 
그 결과를 또 stage3에 넣는 방식이다. 각 stage에서는 해상도를 조절하기 위한 PatchMerging이 수행되는데, 주변의 Patch들을 하나로 
합치므로 이미지의 크기는 1/2씩 줄어든다. 임베딩 차원은 stage별로 각각 [96,192,384,768]이다.   
   
### Swin Transformer Block
Swin Transformer Block은 두 개가 연속으로 붙어있는 것이 기본 구조이다. 일반적인 Window Attention을 진행하고 다음 블럭에서
Shifted Window Attnetion을 진행한다. 즉, 각 단계별 블럭 갯수인 [2,2,6,2]는 각각 
[W-MSA & SW-MSA, W-MSA & SW-MSA, W-MSA & SW-MSA & W-MSA & SW-MSA & W-MSA & SW-MSA, W-MSA & SW-MSA]인 셈.   
   
블럭은 2층의 MLP가 GELU함수와 함께 사용되고 각 MSA모듈과 MLP이전에는 LayerNorm레이어가 적용되며, 각 모듈 이후에는 
Residual Connection이 적용된다. 
