---
layout: post
title: 
description: |
  Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, 
  Zheng Zhang, Stephen Lin, Baining Guo, 
  Microsoft Research Asia
hide_image: true
tags:
  - review
published: true
---

# Swin Transformer: Hierarchical Vision Transformer using Shifted Windows
[Link](https://arxiv.org/pdf/2103.14030.pdf)
* * *

## Abstract
텍스트와 이미지의 차이점(**스케일변화 및 고해상도**)으로 인해 Transformer를 비전 분야에 적용하는 것이 어렵다. SwinTransformer는
이런 문제를 해결하기 위해 **Shifted Windows를 사용하는 계층적인 Transformer**를 제안한다. 어텐션 영역을 겹치지 않는 윈도우로
제한하여 **효율성을 높이고, 이미지 크기에 대해 선형적인 계산 복잡성을 제공한다. 또한, 다양한 스케일에 대한 모델링 유연성을 
갖고 모든 MLP 아키텍처에 적용가능**하다.   

## Introduction
![image](https://user-images.githubusercontent.com/69246778/222346760-a1ad084e-5294-4eed-b023-1323fa27cc50.png)   
기존 Vision Transformer는 저해상도의 단일 피쳐맵에 대해서 전역적인 어텐션을 수행했다. 반면, Swin은 계층적인 피처맵을 구성하며
각 윈도우 내에서만 어텐션을 수행하므로 이미지 크기에 대해 선형적인 계산 복잡도를 가진다.   
   
   
![image](https://user-images.githubusercontent.com/69246778/222346677-c8d61b2a-dff7-4305-b071-6007ec72ae0c.png)   
Swin Transformer의 핵심 디자인 중 하나는 **Shifted Window**이다. 먼저 일반적인 Window Partioning으로 셀프 어텐션을 수행하고 
다음 레이어에서 partion을 이동시킨 후 새로운 window기반의 셀프 어텐션을 수행한다. 새로운 window는 이전 레이어의 윈도우간 경계를 
포함한다.   
   
   
![image](https://user-images.githubusercontent.com/69246778/222346075-ed7cbd4c-d8a6-4986-a5f9-7b3883d43fad.png)   
![image](https://user-images.githubusercontent.com/69246778/222346300-0c66af74-d872-4582-b3dd-2f3bf1913fe4.png)   
이전의 Sliding Window기반의 어텐션은 쿼리마다 다른 key를 공유하지만 Shifted Window는 동일한 key를 공유하므로 하드웨어에서 메모리 엑세스가 
용이하다. 위의 Table5,6에서도 알 수 있듯이 모델링 파워는 비슷하지만 FPS가 훨씬 개선된 것을 확인할 수 있다.   
   
   
## Related Work
* **CNN and variants**   
CNN은 컴퓨터 비전의 대표적인 네트워크 모델로 사용된다. AlexNet, VGG, GoogleNet, ResNet, DenseNet, HRNet, 
EfficientNet등 깊고 효과적인 합성곱 신경망 구조가 제안되었다. 그 밖에 Depthwise convolution과 deformable convolution과 같은 개별 합성곱
레이어 개선에 대한 연구도 진행되고 있다. CNN은 여전히 중요한 비전 네트워크지만 Tranformer같은 구조들이 기본적인 비전 인식 태스크에서 매우
좋은 성능을 달성했고, 이것이 비전과 언어를 통합 모델링 할 수 있을 것으로 기대한다.   
   
* **Self-attention based backbone architectures** 
이전 논문들('Local relation networks for image recognition','Stand-alone self attention in vision models',
'Exploring self-attention for image recognition')에서 특정 영역 내에서만 어텐션을 수행하는 방법을 제안하여 ResNet보다 성능을 달성했다. 
그러나 이들은 메모리 엑세스에 비용이 많이 드므로 convolution 네트워크보다 더 큰 지연시간을 발생시킨다. Swin Transformer는 
**Shift Windows**를 사용하여 하드웨어에서 더 효율적인 구현이 가능하도록 한다.
   
* **Self-attention/Transformers to complement CNNs**   
CNN에 self-attention layer나 transformer를 추가하는 연구도 있다. Swin은 기본적인 시각적 피처 추출에 transformer를 적용한다. 
   
* **Transformer based vision backbones**
우리 연구와 가장 관련 있는 것은 Vision Transformer부분이다. 기존 ViT는 중간 크기 정도의 이미지 패치에 Transformer를 적용한다. 높은 
성능에 달성하는 대신 대규모의 훈련 데이터셋(JFT-300M)이 필요하다. DeiT는 ViT가 적은 데이터셋(ImageNet1K)를 사용하여도 효과적인 성능을 내는
방법 몇 가지를 소개한다. 그러나, ViT는 이미지 해상도가 높은 경우엔 적합하지 않다. 왜냐하면 복잡성이 이미지의 크기에 따라 제곱으로 증가하기 
때문이다.
