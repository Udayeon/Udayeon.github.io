---
layout: post
title: 
description: |
  Wu, H., Xiao, B., Codella, N., Liu, M., Dai, X., Yuan, L., & Zhang, L. (2021). Cvt: Introducing convolutions to vision transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision (pp. 22-31).
hide_image: true
tags:
  - review
published: true
---

# CvT: Introducing Convolutions to Vision Transformers
[Link](https://arxiv.org/abs/2103.15808)
* * *
ViT에 Convolution을 도입하며 성능을 개선한다. 아래의 두 가지 방법이 사용된다.
* **Convolution Token Embedding이 포함된 계층 구조**
* **Convolutional projection**
위의 방법을 통해 CNN의 특성(shift,scale,distortion invariance)과 ViT의 특성(동적 어텐션, global context,일반화)
을 모두 유지할 수 있다. 또한, ViT의 **위치인코딩을 제거**해 높은 해상도에서의 비전 작업도 간단한 구조로 수행 가능해졌다.

# 1. Introduction
Vision Trnasformer는 대규모 데이터셋에서는 성능이 매우 좋은데 적은 양의 데이터로 훈력할 때는 CNN이 더 나은 성능을 보인다. 그 이유는,
CNN이 **지역 수용장(local receptive fields), 공유 가중치(shared weights), 공간적 하향 샘플링(spatial subsampling)** 을 사용해
이미지의 지역적 구조를 캡처할 수 있어서 약간의 이동이나 크기 및 왜곡에 대해 불변성을 가질 수 있기 때문이다. 또한 **CNN의 계층적인 
구조**는 다양한 스케일에서 context를 파악할 수 있게 해준다.   
본 논문에서는 위와 같은 **CNN의 특성을 ViT에 접목**시켜 성능을 향상시키다. 이는 성능 뿐만 아니라 매개변수 측면에서도 매우 효율적이다.   
CvT디자인은 다음 두 가지 기법을 제안한다.   
* **Transformer를 여러 단계로 분할해 계층적 구조를 형성한다. 그리고 각 단계의 시작은 합성곱 토큰 임베딩으로 구성되고 
이후 레이어 정규화를 수행한다.**
* **Transformer 모듈에서 각 self-attention블록 이전 단계인 linear projection을 Convolutional projection으로 대체한다.**

# 2. Related Work
## 2.1. Vision Transformers.
데이터 양이 충분한 경우라면 ResNet이나 EfficientNet이랑 유사한 최신 성능을 달성할 수 있다. 이미지를 겹치지 않는 패치로 분할하고 
이렇게 만들어진 토큰을 모델링하기 위해 MHSA 및 Positionwise FeedForwardNetwork를 적용한다. 다음은 다양한 방식의 ViT이다.  
* **DeiT**   
ViT의 효율적인 학습 및 distillation을 탐구한다.   
* **CPVT**   
Conditional Position encodings Visual Transformer, ViT에서 기존에 사용하던 미리 정의된 위치 임베딩을 **Conditional 
Position encodings(CPE)** 으로 대체한 모델이다.   
* **TNT**   
Transformer-iN-Transformer, 외부 트랜스포머와 내부 트랜스포머로 구성되어 있는데 외부 트랜스포머에서는 패치 임베딩을 처리하고
내부 트랜스포머에서는 픽셀 임베딩 간의 관계를 모델링한다. 이를 통해 **패치 수준과 픽셀 수준 모두를 모델링** 할 수 있다.   
* **T2T**   
Tokens-to-Token, **슬라이딩 윈도우 내의 여러 토큰을 하나의 토큰으로 연결**하여 ViT토큰화를 개선하는데 사용된다.   
* **PVT**   
합성곱 없이 **트랜스포머를 여러 단계로 모델링**하여 Dense 예측에 용이하다.   
   
아래 표로 정리할 수 있다.   
![image](https://github.com/Udayeon/Udayeon.github.io/assets/69246778/ace0dc06-8de5-487a-b92d-2f3c65c3ab99)   
![image](https://github.com/Udayeon/Udayeon.github.io/assets/69246778/9c6dc06e-3d3d-46cc-b448-af5a17bdd494)   
위의 소개된 모델들 간의 성능 비교. 위치 인코딩, 토큰 임베딩 방식, projection 방식, 트랜스포머 구조 등의 차이에 따라 결과가 달라짐을
확인할 수 있다. 

## 2.2. Introducing Self-attentions to CNNs.
* **Non-local networks**   
장거리 종속성을 포착하기 위해 설계되었다. 거리에 상관없이 관계성을 포착할 수 있다.   
* **local relation networks**   
local relation network는 가중치 집합을 선택할 때 local window내의 픽셀과 피처의 구성적인 관계의 유사성을 기반으로 한다.
Convolution layers가 공간적으로 가까운 input feature에 대해 고정된 가중치 집합을 선택하는 것과는 다르다. 적응적인 가중치 선택을
통해 인식 작업에서 기하학적인 이점을 가질 수 있다.   
* **BoTNet**   
ResNet의 마지막 세 bottleneck에서 **Spatial convolution을 global self-attention으로 대체**하여 성능을 개선했다.   

## 2.3. Introducing Convolutions to Transformers.   
* **Pay less attention with lightweight and dynamic convolutions**   
다중 헤드 어텐션을 convolution layer로 대체   
* **Lite transformer with long-short range attention**   
병렬적으로 convolution layer를 추가  
* **Conformer: Convolutionaugmented transformer for speech recognition.**   
연속적으로 convolution layer를 추가   
* **Evolving attention with residual convolutions**   
Convolution으로 변환 된 residual connection을 통해 어텐션 맵을 다음 레이어로 전파한다.

# 3. Convolutional vision Transformer
![image](https://github.com/Udayeon/Udayeon.github.io/assets/69246778/5f5240b6-25a3-4991-a663-21c189d98233)
전체 아키텍쳐는 위의 그림과 같다. ViT에 **Convolution Token Embedding**과 **Convolution Projection**을 도입했다.   
**(a)** Deep residual learning for image recognition과 Object recognition with gradient-based learning에서 계층 
구조를 참조하여 **3개의 stage**로 구성한다. 그리고 각 단계는 **Convolution Token Embedding**과 **Convolutional Transformer Block**
두 부분으로 구성된다.   
**(b)** Convolutional Transformer Block의 아키텍처를 보여주는데 여기서 **Convolutional Projection**이 수행된다.   

## 3.1. Convolutional Token Embedding
Convolution Token Embedding 레이어를 사용하면 Convolution연산의 매개 변수들 조정해서 토큰의 차원이나 갯수 등을 조절할 수 있다.
이를 통해 토큰 시퀀스 길이를 감소 시키면 토큰의 피처 차원이 증가된다. 그럼 CNN 레이어처럼 복잡한 패턴의 피처도 나타낼 수 있고 더 
넓은 영역에 걸쳐 작용할 수 있다.

## 3.2. Convolutional Projection for Attention
original 위치 기반의 선형 projection을 깊이 기반의 분리 가능한 컨볼루션으로 대체해 Multi-Head Self-Attention한다.   
* 3.2.1. Implementation Details   
![image](https://github.com/Udayeon/Udayeon.github.io/assets/69246778/9555592c-a452-4726-ad80-850f8ec3a706)   
그림 3-(a)는 ViT에서 사용되는 original 위치 기반 linear projection을 보여주고 3-(b)가 Convolutional projection이다.   
식으로 표현하면 다음과 같다.   
![image](https://github.com/Udayeon/Udayeon.github.io/assets/69246778/dcb02580-c285-41fb-861b-1ecdec09efae)
**i stage의 input 토큰들을 2D 토큰맵으로 reshape한 다음 커널 크기가 s인 2D convolution layer를 사용해 projection한다. 
그리고 마지막으로 Flatten과정을 거쳐 1D으로 펼쳐서 Q/K/V매트릭스에 대한 토큰 입력 q,k,v를 만든다. 
여기서 Convolution layer는 **Depth-wise Conv2d, BatchNorm2d, Point-wise Conv2d로 구현된 깊이별로 분리 가능한 layer이다.**

* 3.2.2. Efficiency Considerations
Convolutional Projection layer의 효율성 이점 두 가지
**1. 효율적인 Convolution 활용**   
기존의 original 위치 기반 projection과 비교하면 아주 적은 수의 매개변수만 추가된다.   
**2. MHSA 연산의 계산 비용 감소**   
stride를 1보다 크게 설정하여 토큰 수를 줄인다. k,v에는 stride 2를 적용하고 q에 stride 1을 적용하면 k,v의 토큰 수가 4배로 줄어들고
MHSA계산 비용도 4배 줄어든다. 그림 3-(c)참조.   

# 4. Experiments
* **Architecture**   
![image](https://github.com/Udayeon/Udayeon.github.io/assets/69246778/a551ebce-37f9-423a-9b94-917b6373a323)   
* **results**   
![image](https://github.com/Udayeon/Udayeon.github.io/assets/69246778/d839cf08-b62f-4507-b469-dec5dac739f4)   
