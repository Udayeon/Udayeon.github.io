---
layout: post
title: 
description: |
  논문정보(발행일,저자 등)
hide_image: true
tags:
  - review
published: false
---

# DPT: Deformable Patch-based Transformer for Visual Recognition
[Link](https://arxiv.org/abs/2107.14467)
* * *
**DPT 모델**은 고정 크기의 패치 대신 **변형 가능한 패치**를 사용하여 이미지에서 특징을 추출한다. 변형 가능한 패치를 통해
객체의 모양 및 크기에 적응적으로 학습하므로 높은 성능을 달성할 수 있다.

# 1. INTRODUCTION
Vision Trnasformer는 일반적으로 입력 이미지를 **'고정된 크기'** 의 패치로 나누고 multi-head self-attention을 통해 
패치 간의 문맥을 파악한다. CNN보다 장거리 의존성을 효과적으로 포착할 수 있고 추출된 feature에 더 많은 의미를 담는다는 장점이 있다.
그러나 Transformer가 주로 활용하는 **'고정된 크기'** 의 패치 분할은 다음과 같은 문제를 일으킬 수 있다.   
a) 서로 다른 이미지에서 크기가 다른 객체들로 있해 전체적인 객체 관련 지역 구조를 포착하기 어렵다.   
b) 같은 객체라도 이미지에 따라 다른 스케일과 회전을 갖는데 고정된 방식으로 이미지를 나누면 같은 객체인데도 다르게 인식할 수 있다.   
![image](https://user-images.githubusercontent.com/69246778/237038936-99a4b878-281d-4f7a-a1fd-adfc1de32420.png)
위의 사진을 보면, 두 사진 모두 독수리인데 패치 크기가 고정되어 있다보니 a)독수리 발톱을 온전히 포착하지 못하고, b)두 사진의 독수리를 
마치 다른 객체인 것처럼 인식한다.   
본 논문에서는 **DePatch**라는 새로운 모듈을 제안한다. 이 모듈을 PVT에 붙여서 변형 가능한 패치로 이미지를 분할하면 기존의 성능보다
2.3% 높은 분류 정확도를 달성할 수 있다.   
![image](https://user-images.githubusercontent.com/69246778/237039006-1905f92a-5c18-47b2-8d80-672e8e2ae917.png)
DePatch를 이용하면 위의 사진처럼 객체를 더욱 잘 포착하고 그 특징이 더 명확하게 담긴다. 



# 2. RELATED WORK
## 2.1. Vision Transformer
* **Non-Local block and its variants**   
CNN은 공간적으로 가까운 부분만 고려되고 RNN은 시간적으로 가까운 부분만 고려되어 장거리인 경우 문맥을 포착하기 어렵다. 
이를 해소하기 위해 지역에 구애 받지 않는 Non-Local Network가 제안되었다. Gcnet,Ccnet,Non-local neural networks   
   
* **Only Vision Transformer**   
ViT
   
* **DeiT**   
ImageNet에서 훈련된 모델 성능을 향상 시키기 위해 복잡한 학습 스케줄링과 knowledge distillation을 사용한다.   
   
* **CNN + transformer**   
Explicit Position Encodings for Vision Transformers, Bottleneck transformers   
   
* **고해상도 피처맵 유지**   
Transformer in transformer, Swin Transformer, Pyramid vision transformer   
   
* **지역성에 편향 된 매개 변수 추가**   
ConViT, detr with spatially modulated co-attention   
   
* **무차별적으로 패치 임베딩 모듈 설계**   
Tokens-to-token vit   

## 2.2. Deformable-Related Work
고정된 패턴을 수정해서 적응적으로 만들면 성능 개선에 효과적이다. 방법 2가지가 있다.   
* **Attention 기반 방법**   
Gcnet, Squeeze-and-excitation networks, Ccnet, Non-local neural networks   
   
* **Offset 기반 방법**   
Deformable convolutional network, Recurrent attention convolutional neural network, Glance and focus, Deformable convnets v2, Deformable DETR

# 3. METHOD
## 3.1 Preliminaries: Vision Transformer
패치임베딩과 multi-head self attention의 방정식 표현   
![image](https://github.com/Udayeon/Udayeon.github.io/assets/69246778/06343b97-d382-4efc-a870-061d0f5cef06)
![image](https://github.com/Udayeon/Udayeon.github.io/assets/69246778/c5b754d5-3f93-4b0b-84f1-f30860636a5a)
![image](https://github.com/Udayeon/Udayeon.github.io/assets/69246778/3867b034-73c5-4248-9316-543c2cb1c482)

## 3.2. DePatch Module
3.1의 패치 임베딩은 패치 크기가 고정적이므로 객체를 더 잘 지정하고 기하학적 변형을 처리하기 위해선 변형 가능한 패치 임베딩이 필요하다.   
   
**1. 각 패치의 위치,크기를 input에 기반하여 예측된 파라미터로 변환**   
![image](https://github.com/Udayeon/Udayeon.github.io/assets/69246778/9f7072e7-b756-41d4-b295-6d37ae2a3173)   
* 1-a. 위치 : 원래 중심점 주변으로 이동할 수 있도록 오프셋(𝛿𝑥, 𝛿𝑦) 예측   
* 1-b. 크기 : 고정된 패치 크기 s를 예측 가능한 sh와 sw로 대체   
그럼 아래와 같이 새로운 patch위치가 결정된다. 이때 (𝛿𝑥, 𝛿𝑦,sh, sw)는 패치마다 다를 수 있다.   
![image](https://github.com/Udayeon/Udayeon.github.io/assets/69246778/7eaa0772-8152-42c4-b5b5-fd57fd63416a)   
   
**2. 예측된 영역과 함께 임베딩**   
![image](https://github.com/Udayeon/Udayeon.github.io/assets/69246778/6193a70c-1095-433b-aed9-42ae3eb10939)   
* 모든 패치에 대해 (𝛿𝑥, 𝛿𝑦,sh, sw)를 빽빽하게 예측한 다음, 그 영역으로 임베딩한다.   
* Figure2를 보면, 원래 패치에 offset과 scales을 적용해서 새로운 패치를 만들고 이걸 임베딩했다.   

**3. 새로운 패치 영역이 결정된 후에 각 패치에 대한 특징을 추출한다.**
* 문제는 영역의 크기가 모두 제각각이고 예측된 좌표가 정수로 안 떨어진다는 것이다. (figure2-(b)보면 패치 크기도 달라지고 좌표도 정수로 안떨어짐)   
* 위의 문제를 샘플링과 보간으로 해결한다.   
* 3-a. 샘플링 : 새로운 패치 영역 내에 k x k 개의 점을 균일한 간격으로 샘플링하고 샘플링 된 각 점의 피처를 flatten 및 임베딩한다.   
![image](https://github.com/Udayeon/Udayeon.github.io/assets/69246778/3f1f3a10-47d8-46fb-b1b2-e0bf7e631c5e)   
샘플링된 점의 위치 표현은 위와 같고 이 점들의 특징 a를 이용하여 새로운 패치를 나타내면 아래 식과 같이 표현할 수 있다.   
   
![image](https://github.com/Udayeon/Udayeon.github.io/assets/69246778/141f6f64-35e3-45e9-876b-a0cb7b8913e2)   
i번째 패치는 그 패치를 구성하는 k x k의 균일 간격 점들의 피처를 concat하고 가중치를 곱한 다음 bias를 더한 것으로 나타낼 수 있음.   
   
* 3-b. 보간법 : 3-a에서 샘플링한 점들의 인덱스(p_x , p_y)는 보통 소수점으로 표현되므로 이차 보간법을 사용한다.   
![image](https://github.com/Udayeon/Udayeon.github.io/assets/69246778/6493d505-32dd-4173-807e-0625cb89f926)      
![image](https://github.com/Udayeon/Udayeon.github.io/assets/69246778/e6095842-ee03-48ce-99d5-2cbd18dc56d8)   
(qx,qy)가 (px,py)에 가까울수록 1에 가깝고 멀수록 0에 가까워지는 G를 이용해 해당 샘플링 포인트의 위치를 추정할 수 있다. 


