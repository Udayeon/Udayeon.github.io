---
layout: post
title: Swin Transformer; V2 Scaling Up Capacity and Resolution

description: >
  
tags:
  - review
use_math : true
comments : true
author: Udayeon

published: true
---

# Swin Transformer V2: Scaling Up Capacity and Resolution
[Ze Liu 외 다수... (2021)](https://arxiv.org/pdf/2111.09883v1.pdf)
* * *

[State-of-the-Art](https://paperswithcode.com/sota)
![image](https://user-images.githubusercontent.com/69246778/148172661-2a2986b5-9276-4e5d-b5d4-34489695cc19.png)
![image](https://user-images.githubusercontent.com/69246778/148026048-000ade61-6490-4740-a6e6-5764fb23e7ef.png)

**Object Detection COCO dataset**과 **Segmentation ADE20K dataset**에서 가장 높은 성능을 보이는 
마이크로소프트의 **Swin Transformer v2(2021)** 에 대한 리뷰.
   
본 논문은 자연어처리에 주로 사용되는 **Transformer**개념([Attention Is All You Need](https://arxiv.org/pdf/1706.03762.pdf)에서 transformer과 attention에 
대해 소개함)을 **object detection**에 도입한 것으로, 단어가 아니라 **이미지**를 input으로 삼는다.
그렇다고 해서 본 논문이 최초의 이미지 Transformer인 것은 아니고, **Vision Transformer는**
[Vision Transformer(ViT), Google(2020)](https://openreview.net/pdf?id=YicbFdNTTy)에서
먼저 소개된바 있다. 본 논문은, **Swin(Shifted Windows)** 기법을 사용하는데, 
이는 [Swin Transformer: Hierarchical Vision Transformer using Shifted Windows(2021.3)](https://arxiv.org/pdf/2103.14030v1.pdf)에서 제안되었으며 
이것의 두 번째 version이라 할 수 있다.
 
## Attention Is All You Need
먼저 **Transformer**에 대해 알아보자. **Transformer**는 **Seq2Seq**모델과 유사하게 **인코더(Encoder)-디코더(Decoder)** 구조를 갖는다.   
- [Seq2Seq](https://wikidocs.net/24996) : 입력된 시퀀스로부터 다른 도메인의 시퀀스를 출력하는 모델. 예를들어, 챗봇을 만들 때 입력 시퀀스는 '질문'으로하고 출력 시퀀스는 '대답'으로 설정하는 것. 또는 '문장'을 입력받아 '번역문'으로 출력하는 것 등을 말한다.   
![image](https://user-images.githubusercontent.com/69246778/148022076-2b5fdc23-4966-4a07-8952-5b58acbbdb2d.png)
**I am student**을 입력으로 넣으면 **번역된 문장**이 출력된다.   
      
![image](https://user-images.githubusercontent.com/69246778/148022171-7e52dac6-93d5-4097-a322-50196c03a907.png)
기계 번역기를 살펴보면 위와 같이 **인코더와 디코더** 두 개의 모듈로 구성되어 있다. (입력과 출력의 size가 다를 경우 이런게 필요함. 번역같은거..)
- 인코더 : input을 암호화시켜주는 논리회로. 위와 같은 경우, 입력 받은 단어 정보를 압축해서 하나의 벡터로 만들어줌.
- 디코더 : 인코더가 암호화시킨 정보를 풀어서 출력해줌. 입력 받은 단어 정보를 가지고 번역된 단어로 풀어서 순차적으로 출력함.
   
다시 인코더와 디코더를 분해하면 각각은 RNN(순환신경망)으로 구성되어 있다.
![image](https://user-images.githubusercontent.com/69246778/148022638-5c95eec8-a7aa-42af-8c0d-77c2a5a86dfa.png)
   
Transformer와 Seq2Seq2의 차이점은 **Transformer가 다수의 인코더(encoder's')와 디코더(decoder's')들이 중첩된 구조라는 것이다.** 
![image](https://user-images.githubusercontent.com/69246778/148023376-1410cdfd-497c-40c1-b60b-6be6d0dcb5e3.png)
   
[참조](https://blog.pingpong.us/transformer-review/)
   
## AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE (Vision Transformer(ViT))
자연어 처리에 사용되던 **Transformer**를 어떻게 image에 적용할 수 있는지 생각해보자. 이미지를 처리할 때는 다음과 같이 **이미지를 마치 단어처럼 
patch로 다루면 된다.**
![image](https://user-images.githubusercontent.com/69246778/148028742-46b8c694-f248-4ffe-b02f-8b4831ba586d.png)
[출처](https://engineer-mole.tistory.com/133)
   
각 패치를 단어처럼 다루기 때문에 실제로는 패치를 벡터에서 **Flatten**해주어야 한다. 각 패치에 대해 모두 **Flatten**을 적용하면 다음과 같다.
![image](https://user-images.githubusercontent.com/69246778/148029062-1873c6a0-ff06-435c-a693-c4d05ab9aed4.png)
![image](https://user-images.githubusercontent.com/69246778/148029034-e39ffb6e-5255-46d1-87a9-10947f96aedb.png)
   
![image](https://user-images.githubusercontent.com/69246778/148494168-42b48bdd-e314-4b05-add5-7abf018fb7fb.png)
논문에서 제시하는 로직. 이미지를 여러개의 **16x16사이즈의 패치**로 자른 후에 각 패치를 **Flatten**하고 패치마다 **Position Embedding**을 더한다.
그리고 그 결과물을 **Transformer Encoder**에 입력한다. Transformer Encoder의 구조를 보면, **Embedded Patches**를 **Normalization**하고 


## Swin Transformer: Hierarchical Vision Transformer using Shifted Windows
**기존의 ViT에서는 이 patch의 사이즈를 16x16으로 고정**했다.(ViT논문 제목부터가 16x16 어쩌구임) 
반면, **Swin**에서는 서로 다른 크기의 mask를 계층적으로 사용한다.

# Swin Transformer V2: Scaling Up Capacity and Resolution




