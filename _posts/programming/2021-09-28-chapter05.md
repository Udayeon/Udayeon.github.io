---
layout: post
title: (Python) 파이썬과 Opencv를 이용한 컴퓨터 비전 학습 Chapter05
description: |
  Opencv3 computer vision with python cookbook
hide_image: true
tags:
  - programming
published: true
---

Chapter05.딥러닝

# 5.1. 이미지를 텐서/블롭(blobs)으로 표현
* * *
딥러닝은 한번에 많은 이미지를 처리하는데, 이런 여러 이미지의 묶음을 **배치**, 배치를 한번에 처리하는 것을 **일괄처리**라 한다.
opencv의 딥러닝 기능은 (배치에 포함된 이미지의 수N, 채널의 수C, 이미지의 높이H, 이미지의 너비W)로 구성된 4차원 텐서로 동작하므로 
배열 형태의 이미지 input을 tensor자료형으로 변환해주어야 한다. 변환 과정은 다음과 같다.   
   
* 이미지를 [부동 소수점](https://codetorial.net/articles/floating_point.html) 으로 변환한다.
* 필요한 경우 채널의 순서를 BGR에서 RGB로 변환한다.
* HWC이미지를 CHW텐서로 변환한다.
* CHW텐서에 차원을 추가해 **NCHW**로 만든다.
[배열 전환 참고](https://pybasall.tistory.com/124)

```py
 #필요한 모듈을 불러온다
import cv2   
import numpy as np

#BGR채널의 이미지를 컬러로 불러온다.
image_bgr = cv2.imread('../data/Lena.png', cv2.IMREAD_COLOR)
print(image_bgr.shape)

#이미지를 4차원 부동소수점 텐서로 변경
image_bgr_float = image_bgr.astype(np.float32)  #위에서 불러온 이미지의 데이터 형태를 float32형태로 변환
                                                #float32는 32bits의 실수
image_rgb = image_bgr_float[..., ::-1]  #BGR을 RGB로 변경
tensor_chw = np.transpose(image_rgb, (2, 0, 1)) #image_rgb(512높이,512너비,3채널) --> tensor_chw(3채널,512높이,512너비)
tensor_nchw = tensor_chw[np.newaxis, ...]    #(3,512,512)에 새로운 축 하나를 추가 --> (1N,3C,512H,512W)

print(tensor_nchw.shape)
```
```
>>> (1,3,512,512)
```

# 5.2. Caffe,Torch,텐서플로 형식의 딥러닝 모델 불러오기
* * *
opencv에서 dnn모듈을 사용하면 Caffe,Torch,TensorFlow 세가지 프레임워크로 사전 훈련된 네트워크를 불러올 수 있다. 

```py
import cv2
import numpy as np

#Caffe 모델
net_caffe = cv2.dnn.readNetFromCaffe('../data/bvlc_googlenet.prototxt', 
                                     '../data/bvlc_googlenet.caffemodel')
#Torch 모델
net_torch = cv2.dnn.readNetFromTorch('../data/torch_enet_model.net')

#tensorflow모델
net_tensorflow = cv2.dnn.readNetFromTensorflow('../data/tensorflow_inception_graph.pb')
```

# 5.3. 모든 레이어에 대한 입/출력 텐서 형태 가져오기
* * *
DNN에서 Forward pass를 수행하는 동안 데이터 형태에 대한 정보가 필요한 경우가 있다. 다음의 함수로 추론(훈련된 신경망으로 구체적인 작업을 수행하는 것)없이 모든 텐서의 형태를 얻을 수 있다.



```py
import cv2
import numpy as np

net = cv2.dnn.readNetFromCaffe('../data/bvlc_googlenet.prototxt', 
                               '../data/bvlc_googlenet.caffemodel')ㅣ
if not net.empty(): #네트워크에 어떠한 layer도 포함되지 않으면 empty함수는 True를 반환한다.즉, layer가 비어있지 않다면 
    print('Net loaded successfully\n')  #'Net loaded successfully'출력
    
print('Net contains:') 
for t in net.getLayerTypes(): #네트워크에 포함된 레이어들의 유형에 대한 정보 출력
    print('\t%d layers of type %s' % (net.getLayersCount(t), t)) #get.LayerCount : 레이어 유형을 입력받아 해당 유형의 레이어 수 반환

layers_ids, in_shapes, out_shapes = net.getLayersShapes([1, 3, 224, 224]) #예제수, 채넔, 너비, 높이 4개의 정수를 받아 모든 텐서의 형태를 계산하는 함수
                                                                          #layers_ids : 레이어 식별자 목록
                                                                          #in_shapes : 각 레이어에 대한 입력 텐서 형태 목록
                                                                          #out_shapes: 각 레이어에 대한 출력 텐서 형태 목록
                                                                          

layers_names = net.getLayerNames()  #layer 이름 목록 반환

print('Net layers shapes:')
for l in range(len(layers_names)):
    in_num, out_num = len(in_shapes[l]), len(out_shapes[l])
    print('Layer "%s" has %d input(s) and %d output(s)' % (layers_names[l], in_num, out_num)) #layer이름, input수, output수 
    for i in range(in_num):
        print('\tinput #%d has shape' % i, in_shapes[l][i].flatten())
    for i in range(out_num):
        print('\toutput #%d has shape' % i, out_shapes[l][i].flatten())
```

# 5.4. 이미지 사전 처리와 convolution network 추론
* * *
인공신경망 훈련을 위해 다음의 조건이 필요하다
* 네트워크에서 처리할 수 있는 형식과 범위의 입력 데이터가 있어야 함
* 데이터를 네트워크에 적절히 전달해야 함

```py
import cv2
import numpy as np

#이미지를 열고 
image = cv2.imread('../data/Lena.png', cv2.IMREAD_COLOR)
#전처리를 통해 하나의 이미지를 tensor로 전환하는 함수 이용
#blobFromImage(입력이미지, scale factor, (너비, 높이), 감산을 위한 평균값, 채널교환여부, crop)
tensor = cv2.dnn.blobFromImage(image, 1.0, (224, 224),
                               (104, 117, 123), False, False);

#여러 장의 이미지를 tensor로 전환
tensor = cv2.dnn.blobFromImages([image, image], 1.0, (224, 224),
                                (104, 117, 123), False, True);                               

#훈련된 신경망 모델 불러오기
net = cv2.dnn.readNetFromCaffe('../data/bvlc_googlenet.prototxt', 
                               '../data/bvlc_googlenet.caffemodel')                                     
#불러온 모델로 추론 수행
net.setInput(tensor); #설정하고자 하는 텐서 입력
prob = net.forward(); #텐서 반환

#입력 설정 반복을 통해 지정된 레이어명으로 추론 수행
net.setInput(tensor, 'data');
prob = net.forward('prob');
```
* blobFromImamges가 이미지를 tensor로 변환하는 과정
- crop이 True이면 가로세로 비율 유지한 채로 이미지 크기를 조절하고, False
- 크기를 조절한 이미지의 값을 부동소수점으로 변환
- 채널교환여부가 True이면 R과B를 교환, False면 그대로 (디폴트는 False임)
- 이미지의 각 픽셀에서 평균값을 뺌. 인수가 3개이면 채널 각각 따로 빼주고, 인수가 1개면 모든 채널에서 동일한 수를 빼줌
- scale factor를 결과 이미지에 곱해줌
- **3차원 이미지를 NCHW 4차원 텐서로 변환**


# 5.5. 추론 시간과 각 계층의 기여도 측정
* * * 
Forward pass를 수행한 네크워크에서 부동소수점 연상의 수행 횟수, 소비되는 메모리 계산
```py
import cv2
import numpy as np

#Caffe모델 불러오기
model = cv2.dnn.readNetFromCaffe('../data/bvlc_googlenet.prototxt',
                                 '../data/bvlc_googlenet.caffemodel')

#FLOPS계산
#FLOPS: 컴퓨터의 성능을 수치로 나타낼 떄 단위, 컴퓨터가 1초당 수행할 수 있는 부동소수점 연산 횟수
print('gflops:', model.getFLOPS((1,3,224,224))*1e-9) #인수 : (n,c,h,w)기본단위가 나노세크이므로 1e-9곱

#소비 메모리
w,b = model.getMemoryConsumption((1,3,224,224)) #w:가중치 저장에 사용되는 메모리, b:중간 텐서에 사용되는 메모리
print('weights (mb):', w*1e-6, ', blobs (mb):', b*1e-6)

#모의 입력 전달로 Forward pass수행
blob = cv2.dnn.blobFromImage(np.zeros((224,224,3), np.uint8), 1, (224,224))
model.setInput(blob)
model.forward();

#전체 시간 출력
total,timings = model.getPerfProfile()
tick2ms = 1e3/cv2.getTickFrequency()
print('inference (ms): {:2f}'.format(total*tick2ms))

#각 레이어별 추론 시간 출력
layer_names = model.getLayerNames()
print('{: <30} {}'.format('LAYER', 'TIME (ms)'))
for (i,t) in enumerate(timings):
    print('{: <30} {:.2f}'.format(layer_names[i], t[0]*tick2ms))
```

# 5.6. GoogleNet/Inception, ResNet모델로 이미지 분류
* * *
컴퓨터 비전에서의 분류는 input이미지가 특정 카테고리에 속할 확률을 추정하는 것이다.
```py
import cv2
import numpy as np

#classify 정의
def classify(video_src, net, in_layer, out_layer, 
             mean_val, category_names, swap_channels=False):
    cap = cv2.VideoCapture(video_src)

    t = 0
    
    while True:
        status_cap, frame = cap.read()
        if not status_cap:
            break

        if isinstance(mean_val, np.ndarray):
            tensor = cv2.dnn.blobFromImage(frame, 1.0, (224, 224),
                       1.0, False);
            tensor -= mean_val
        else:
            tensor = cv2.dnn.blobFromImage(frame, 1.0, (224, 224),
                                   mean_val, swap_channels);
        net.setInput(tensor, in_layer);
        prob = net.forward(out_layer);

        prob = prob.flatten()

        r = 1
        for i in np.argsort(prob)[-5:]:
            txt = '"%s"; probability: %.2f' % (category_names[i], prob[i])
            cv2.putText(frame, txt, (0, frame.shape[0] - r*40), 
                        cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2);
            r += 1

        cv2.imshow('classification', frame)
        if cv2.waitKey(1) == 27:
            break
        
    cv2.destroyAllWindows()
    cap.release()


with open('../data/synset_words.txt') as f:
    class_names = [' '.join(l.split(' ')[1: ]).rstrip() for l in f.readlines()]
    
googlenet_caffe = cv2.dnn.readNetFromCaffe('../data/bvlc_googlenet.prototxt', 
                                           '../data/bvlc_googlenet.caffemodel')

classify('../data/shuttle.mp4', googlenet_caffe, 'data', 'prob', (104, 117, 123), class_names)  


resnet_caffe = cv2.dnn.readNetFromCaffe('../data/resnet_50.prototxt', 
                                           '../data/resnet_50.caffemodel')
mean = np.load('../data/resnet_50_mean.npy')

classify('../data/shuttle.mp4', resnet_caffe, 'data', 'prob', mean, class_names)

with open('../data/imagenet_comp_graph_label_strings.txt') as f:
    class_names = [l.rstrip() for l in f.readlines()]

googlenet_tf = cv2.dnn.readNetFromTensorflow('../data/tensorflow_inception_graph.pb')

classify('../data/shuttle.mp4', googlenet_tf, 
         'input', 'softmax2', 117, class_names, True)
```

# 5.7. SSD모델로 객체 검출
* * *
Single Shot Detection으로 차량 검출

```py
import cv2
import numpy as np

model = cv2.dnn.readNetFromCaffe('../data/MobileNetSSD_deploy.prototxt',
                                 '../data/MobileNetSSD_deploy.caffemodel')

CONF_THR = 0.3
LABELS = {1: 'aeroplane', 2: 'bicycle', 3: 'bird', 4: 'boat',
          5: 'bottle', 6: 'bus', 7: 'car', 8: 'cat', 9: 'chair',
          10: 'cow', 11: 'diningtable', 12: 'dog', 13: 'horse',
          14: 'motorbike', 15: 'person', 16: 'pottedplant',
          17: 'sheep', 18: 'sofa', 19: 'train', 20: 'tvmonitor'}
          
video = cv2.VideoCapture('../data/traffic.mp4')

c=0
while True:
    ret, frame = video.read()
    if not ret: break
        
    h, w = frame.shape[0:2]
    blob = cv2.dnn.blobFromImage(frame, 1/127.5, (300*w//h,300),
                                 (127.5,127.5,127.5), False)
    model.setInput(blob)
    output = model.forward()
    
    for i in range(output.shape[2]):
        conf = output[0,0,i,2]
        if conf > CONF_THR:
            label = output[0,0,i,1]
            x0,y0,x1,y1 = (output[0,0,i,3:7] * [w,h,w,h]).astype(int)
            cv2.rectangle(frame, (x0,y0), (x1,y1), (0,255,0), 2)
            cv2.putText(frame, '{}: {:.2f}'.format(LABELS[label], conf), 
                        (x0,y0), cv2.FONT_HERSHEY_SIMPLEX, 1, (0,255,0), 2)
    
    c += 1
    if c == 100:
        cv2.imwrite('/home/alexeysp/projects/recipes/figures/ch5_car_detections.png', frame)
    
    cv2.imshow('frame', frame)
    key = cv2.waitKey(3)
    if key == 27: break
        
cv2.destroyAllWindows()                            
```

# 5.8. FCN모델을 사용해 장면 분할
* * *
Fully Convolution Network, 장면에 대한 이해가 필요할 때 사용한다.
[Caffe model](https://github.com/shelhamer/fcn.berkeleyvision.org/blob/master/voc-fcn8s/caffemodel-url)
필요한 Caffemodel은 위에서 다운로드 받았다.

```py
import cv2
import numpy as np
import matplotlib.pyplot as plt
%matplotlib auto

model = cv2.dnn.readNetFromCaffe('../data/fcn8s-heavy-pascal.prototxt',
                                 '../data/fcn8s-heavy-pascal.caffemodel')
                                 
                                 


