---
layout: post
title: VisualSLAM
description: |
  [K-mooc 자율주행자동차기술(국민대학교) 강의](http://www.kmooc.kr/courses/course-v1:KMUk+CK-KMUK_02+2021_1/course/)
  
tags:
  - autonomous
use_math : true
comments : true
author: Udayeon
published: true
---

Visual SLAM

* 1. Visual SLAM 기술 개요
  * 1.1 Visual SLAM
  * 1.2 특징 검출과 특징 매칭
  * 1.3 카메라 자세 추정
  * 1.4 특징점 탐색 실습
* 2. VisualSLAM 기술 기초
  * 2.1 초기 카메라 추정
  * 2.2 카메라 트래킹
  * 2.3 이중 시점 분석 실습
* 3. VisualSLAM 기술 심화
  * 3.1 국소 지도 확장과 전역 지도 정합
  * 3.2 루프 결합과 카메라 위치 재조정
  * 3.3 카메라 위치 재조정 실습
 
 
# 1. Visual SLAM 기술 개요
* * *

## 1.1 Visual SLAM
* * *

### 1.1.1 SLAM기술이란?
동시적 위치추적 및 지도 작성기술.   
미지의 영역에 대해 환경지도를 작성하는 **Mapping**과 작성된 지도상에서 위치를 추정하는 **Localization**을 동시에 해결하는 알고리즘.
자기 위치가 어딘지 알아야 새롭게 탐사한 영역에 대한 정보를 현재 지도에 정확히 붙일 수 있음.

### 1.1.2. Visual SLAM 이란?
**시각 데이터**를 이용해 동시적 위치추적 및 지도 작성을 하는 알고리즘.

### 1.1.3. Visual SLAM 응용분야
* 자율주행 자동차   
인지 기술로 작동   
EX ) 구글의 웨이모   
지붕에 달린 라이다가 시각 센서로 작동하여 지도 정보가 없는 곳을 지나면 알고리즘으로 지도를 작성(Mapping)하고 이미 지도를 확보한 곳에서는
현재 자동차가 어디에 있는지를 파악함(Localization).   
   
* 증강 현실   
모바일 폰이나 HMD에 달린 카메라가 주변 환경에 대한 시각 데이터를 수집하고 Visual SLAM으로 환경지도와 현재 위치를 얻어낸다. 
그리고 가상 물체를 지도상에 배치해 실제로 있는 것과 같이 표현.   
   
* 로봇청소기   
로봇청소기를 처음 사서 집을 한 번 훑어보게 하면 Visual SLAM이 지도를 작성함. 정확한 지도를 위해 위치 추정도 동시에 진행.   
      
### 1.1.4. 로봇청소기로 알 수 있는 자율주행의 문제   
* Kidnapping   
전원이 꺼진 로봇청소기를 사람이 직접 들어 다른 방에 옮겨 놓고 전원을 켜는 경우.   

* 위치 재조정   
Kidnapping된 로봇이 전원이 꺼지기 전 마지막으로 알고 있던 위치는 잘못된 정보라는 것을 인지하고 지도에서 자신이 자리잡고 있는 새 위치를 빠르게 인식하는 것.   
   
* 업데이트    
자신이 기존에 알고 있던 지도를 현재 상황에 맞에 업데이트하는 것.   
   
**자율주행의 경우** 시동이 꺼지기 전 자동차의 위치와 시동이 다시 켜진 위치가 다르거나 도로의 지형이 바뀌거나 지도에는 없는 큰 
장애물이 나타나는 경우, 자율주행 자동차는 기존의 지도 정보를 업데이트해 가며 안전히 운행해야 함.   

### 1.1.5. Visual SLAM System Component   
* Visual Sensor   
(일반적으로 카메라)이미지 데이터를 수집.   
   
* 특징점 검출과 매칭   
**특징점 검출**은 VisualSLAM System이 영상을 손쉽게 다루기 위해 필요한 영상처리 과정. 한 장을 빠르고 효과적으로 처리하기 위해 
이미지로부터 특징이 되는 지점을 검출하는 과정이고, **특징점 매칭**은  서로 다른 두 영상에서 검출한 특징점 사이의 대응관계를 구하는 과정. 
특징점 검출과 매칭으로 처리된 영상들이 VisualSLAM System에서 활용됨.   
   
**VisualSLAM에서의 활용**    
  - 카메라 자세추정 : 같은 장면을 서로다른 시점에서 바라본 이미지를 활용하여 두 카메라의 상대적 위치를 추정. 추후, 이미지 두 장으로부터 
   삼각측량을 이용해 3차원 위치를 추정할 수 있음 **(3차원 지도 작성)**   
  - 카메라 트래킹 : 카메라가 돌아다니면서 이미지를 얻을 때 이미지의 특징점으로 카메라가 지도 내에 어디 있는지 위치를 추정.   
  - 국소적 지도관리 : 카메라가 돌아다니면서 경계에 다다를 때 지도를 넓히는 것.   
  - 전역 지도관리 : 전체 지형을 둘러볼 때 위상적으로 문제가 없도록 연결고리 형태로 지도 구성.


## 1.2. 특징 검출과 매칭
* * *

### 1.2.1. 특징 검출과 매칭이란?
카메라가 찍은 이미지에 대한 특징을 검출하고 서로 다른 이미지에서 검출된 특징간의 매칭을 계산하는 과정. SLAM을 돌리기 위한 가장
기본적인 데이터 전처리 과정. 

### 1.2.2. 특징 검출
예를 들어, 3차원 장면을 카메라로 찍게되면 너비x높이의 개수만큼 픽셀로 구성된 이미지를 얻게되는데, 이는 **SLAM이 처리하기에 너무 방대함.** 
따라서, SLAM시스템에서 이미지 전체에 대한 데이터를 넘겨주기 보다는 **특징 검출을 통해 이미지를 특징점 위주로 줄인 후** 해당 지점만 SLAM System에 넘겨줌. 그럼 **SLAM System이 가볍고 빠르게(효율적으로)** 돌아갈 수 있음.   

#### 1.2.2.A. 특징점
그렇다면 어떤 점이 특징점이 될 수 있을까?   
**주변의 다른 지점과 비교했을 때 고유하게 인식될 수 있는 지점**을 특징점이라고 함. 보통 특징점은, 육안으로 봤을 때의 모서리 지점(corner point)
에 해당함.   

#### 1.2.2.B. 특징 검출 알고리즘
**Harris corner detection 알고리즘**   
해리스가 제안한 자가유사도 기반의 알고리즘.
1. 다음과 같이 주어진 이미지에 빨간색, 노란색, 녹색으로 표시된 3지점을 살펴봄.   
![image](https://user-images.githubusercontent.com/69246778/126956046-d5156937-f841-447a-8894-ec1034d6dfb3.png)   
   
2. 먼저 빨간색 지점(작은 빨간 박스)을 살펴봄. 빨간색 지점 중심으로 이미지 조각인 패치를 생성(커다란 빨간박스).   
빨간색 패치 내의 한 픽셀을 새로운 지점(주변 지점)으로 삼고 그 지점에 대한 새로운 패치를 생성(커다란 회색박스).   
이때 빨간색 패치와 회색 패치를 비교해 보면 둘이 같아서 구분이 안됨.   
빨간색 패치 내의 다른 지점들도 이런 방식으로 보면 비슷해서 구분 안됨. 따라서 빨간색 지점은 특징점이 될 수 없음.   
![image](https://user-images.githubusercontent.com/69246778/126956090-5c2dea15-f6e3-4a31-9cc7-e20e8f1be531.png)   
   
3. 노란색 지점을 중심으로 패치를 잡고 위와 같은 방식으로 진행함. 만약 edge방향으로 주변 지점을 잡으면 회색패치와 구별이 안됨.   
노란색도 특징점이 될 수 없음.   
![image](https://user-images.githubusercontent.com/69246778/126956123-348a6c47-3473-4639-b950-ba3dfb952e29.png)   

4. 녹색 지점을 중심으로 패치를 잡고 다시 반복. 어떤 지점을 잡아도 주변 지점과 확실하게 구별됨. **특징점**   
![image](https://user-images.githubusercontent.com/69246778/126956171-c0e65828-c76f-47c8-bd24-f091f2275272.png)   
   
**FAST corner detector**, **ORB corner detector**등 다양한 알고리즘이 있음.

### 1.2.3. 특징 매칭
두 이미지에서 따로 뽑은 특징점이 실제로 같은 지점이라고 생각된다면 서로 짝을 지어줌. 특징 매칭은 **두 이미지 사이의 관계 계산**에 쓰임. 
예를 들어, 이미지에 찍힌 내용이 다른 이미지에도 들어 있어 **두 영상이 유사한지 여부**를 측정하거나, **카메라 자세 추정**에 활용.

#### 1.2.3.A. 특징 매칭의 과정
1. 특징묘사 과정   
**특징점을 비교 가능한 자료 형태(특징 묘사자)로 바꿔줌.** 좋은 특징 묘사자는 사람의 눈으로 봤을 때 동일한 지점이라고 생각하는 경우,
비슷한 특징 묘사자를 가지도록 하고 아닌 경우는 다른 특징 묘사자를 가지도록 하는 것. 즉, 어떤 지점이 확대되거나 축소되거나 회전되거나
어파인 또는 투사변환 된다든지 조명 조건이 살짝 달라진 환경에서도 동일 지점은 거의 비슷한 특징 묘사자를 만들어내야 함. 만약, 다른 지점이라면
완전 다른 특징 묘사자를 만들어 내어 확연히 구별할 수 있게 해야 함.   
   
**SIFT** : David Lowe가 제안한 대표적인 특징 묘사자. 특징점 주변에 밝기변화 크기에 대한 중심축을 잡고 그 중심축만큼 회전한 패치를 잡음.
회전된 지점마다 밝기 변화량이 가장 크게 일어나는 Gradient방향을 수집. 그리고 Gradient를 부분별로 8개의 주 방향에 대해 얼마나 많이 누적되어
있는지에 대한 정보를 수집. 이러한 누적 결과가 해당 지점의 특징 묘사자로 쓰임.   
   
**HoG** : 각 지역별 Gradient를 고려   
   
**BRIEF**(임의의 패턴을 이용해서 고른 두 주변 지점에 대해, 상대적으로 어둡고 밝은 패턴을 기록)

2. 특징매칭 과정
특징 묘사자의 **유사도를 정의하고 서로 다른 이미지에서 등장한 유사한 특징끼리 서로 짝을 지어주는 과정**. 
**유사도 측정**은 특징 묘사자 벡터 거리 측정 방식이 주를 이룸. 매칭으로 고려할 쌍이 너무 많으므로 병렬처리나 GPU를 이용한 가속을 
고려해야함. 
또한, 특징 묘사자가 유사하여 짝을 지었다고 하더라도 잘못된 결과가 나올 수 있음. 이런 쌍들은 
**Cycle consistency**나 **Epipolar consistency**를 활용해 걸러낼 수 있음.   
      
**Cycle consistency**: A이미지에서 뽑힌 특징점을 기준으로 B에서 유사한 특징점을 찾는 방식과, B에서 뽑힌 특징점을 기준으로 A에서의 유사한
특징점을 찾는 방식이 다른 경우 Cycle consistency가 깨진 것.   
   
**Epipolar consistenccy**: 삼각측량을 사용할 때 삼각형의 세 점을 항상 같은 평면에 놓여야 한다는 점을 이용해 잘못된 매칭 쌍을 걸러냄.   

## 1.3. 카메라 자세 추정
* * *

### 1.3.1. 카메라 기하 구조(단일 시점 기하)
**단일 시점 기하** : 카메라 한 대가 정의하는 기하구조로 한 점이 3차원 공간에서 빛줄기로 해석됨.   
카메라는 렌즈, 바디, 필름 또는 CCD센서로 구성됨. Camera obscura로부터 만들어짐. 
가장 단순한 카메라의 구조는 빛이 통과하는 핀 홀과 상이 맺히는 이미지만으로 구성된다고 가정. 
이 때 다음과 같이 **초점거리(focal length,핀홀~이미지)와 이미지의 길이비**는, **핀홀과 실제 나무사이의 거리(depth)와 
실제나무의 길이비**와 같다.   
![image](https://user-images.githubusercontent.com/69246778/127080037-bc0947b3-2bb9-4742-bf3e-574a13225652.png)   
   
그런데 상이 뒤집혀 맺히므로 좌표계의 부호를 엄밀히 고려해야 하는 계산상의 불편함이 있으므로 다음과 같이 이미지가 
**카메라의 앞**에 위치한다고 가정. 
![image](https://user-images.githubusercontent.com/69246778/127080091-d22bc88d-0529-4816-a2ac-64f3d7083c56.png)


   
서로 다른 초점거리를 가지는 카메라가 우연히 동일한 영상을 촬영할 경우, 실제 물체와 카메라 사이의 거리는 다름.   
이미지만으로 초점거리를 바로 알아내기는 쉽지 않음. 이를 **카메라 내부 파라미터 추정**이라 함.   
![image](https://user-images.githubusercontent.com/69246778/127081369-633cc254-6f0c-4a4a-8b06-ffd41b0a5066.png)   
   
### 1.3.1.A 카메라 내부 파라미터
주로 주점(Principle Point)의 위치, 초점거리 등을 말하며 주어진 정보가 이미지 뿐일 때 이를 분석해 카메라 내부 설정을 추정하는 것을
카메라 내부 파라미터 추정이라한다. 

* **이미지에 대한 주점, 초점거리를 알 수 없는 경우**
보통 주점은 이미지의 중심인데 만약 카메라가 떨림 보정 기능을 가지고 있다면 이미지를 바로 저장하지 않고 떨림 보정을 위해 
**크롭해서 저장**함.그러면 주점이 이미지의 중심이 아닐수 있음. 또는 **인터넷에서 다운받은 이미지**는 크롭됐는지 여부를 알 수 없으므로
주점이 이미지의 중심인지 확정할 수 없음.   
초점거리를 알 수 없는 경우도 많음. 초점거리가 특징되지 않으면 특징점의 위치를 알아도 핀홀과 특징점을 잇는 빛줄기를 파악할 수 없음. 
즉, 같은 이미지를 얻어도 서로 다른 초점거리로 찍혔다면 3차원 공간에서 **서로 다른 빛줄기로 상이 맺했다고 생각할 수 있음.**
SLAM의 지도 작성과 위치추정이 어려움. 따라서 주점과 초점거리를 정밀하게 알아내는 **카메라 내부 파라미터 추정**이 필요.

* **Calibration**
카메라 내부 파라미터 추정 기능 지원   
* openCV : calibrateCamera() 함수   
* MATLAB : calibration tool box
   
* **카메라 내부 행렬** : 카메라 내부 파라미터를 수학적으로 표현한 행렬. 
![image](https://user-images.githubusercontent.com/69246778/127083128-242ffe31-59fb-4010-ab88-2e510fe967d4.png)   
**f_x, f_y** : 초점거리   
**C_x, C_y** : 주점의 위치   
   
이렇게 알아낸 내부 행렬을 이용해 이미지 상의 한 점을 카메라 좌표계 기준의 3차원 빛줄기로 변환. 

### 1.3.1.B. 카메라 외부 파라미터
SLAM이 만들어 낸 3차원 환경지도는 각 이미지를 찍은 카메라가 3차원에서 어떤 자세를 가지고 있는지에 대한 정보가 필요. 
즉, 카메라가 지도작성 좌표계를 기준으로 어떻게 회전하고 이동하였는지에 대한 정보(카메라가 세상에 놓인 자세)   
카메라의 **회전**과 **이동 정보**를 합친 행렬 rt형태를 띰.
![image](https://user-images.githubusercontent.com/69246778/127083683-ee9f3736-858e-4d0c-b241-a3a52f347433.png)
이를 이용해 카메라 좌표계로 표현된 정보를 월드 좌표계로 변환할 수 있음. 앞에서 이야기한 **내부 파라미터로 한 점을 빛줄기로 변환**
하는 것 까지 이용하면, **이미지의 한 점을 월드 좌표계로 표현된 빛줄기로 해석할 수 있게 됨.** 즉.**이미지 특징점 하나가 어떻게
카메라 위치에서 세상으로 뻗어나가는가를 표현**

* **Calibration**
카메라 외부 파라미터 추정 기능 지원   
* openCV : calibrateCamera() 함수   
* MATLAB : calibration tool box   
   
* **카메라 외부 행렬** 
![image](https://user-images.githubusercontent.com/69246778/127083771-895f5c19-edd5-49f9-89bb-e36b8137d51c.png)
월드 좌표계가 빨간색, 녹색, 파란색 축으로 표현된 것은 현재 카메라의 회전과 위치를 정확히 추정한 결과임.



### 1.3.2. 카메라 기하 구조(이중 시점 기하)
**이중 시점 기하** : 카메라 두 대가 정의하는 기하구조로 삼각 측량을 통한 두 장의 이미지로부터 3차원 인식. 마치 사람의 두 눈이 하는 일과
유사한 동작으로 위치를 추정함. 단일 시점 기하와 거의 같지만, **핀 홀 중심으로 카메라의 3차원 좌표계**를 고려함.   
![image](https://user-images.githubusercontent.com/69246778/127080522-830048ce-d4fb-4894-81aa-6432cbdcb96c.png)   
   
상이 카메라 앞에 맺힌다고 가정. 물체 위의 한 지점을 카메라 좌표계(x,y,z)로 표현. 해당 지점이 초점거리 f에 위치한 상에 맺힌다면
상에 맺힌 지점의 좌표는 (f * x/z, f * y/z, f). 즉, 이미지가 초점거리 f만큼 떨어져 있다면 이미지상의 한 점은 3차원 좌표계로 표시됨.
이처럼 이미지의 한 점이 2차원 좌표계가 아닌 3차원 좌표계로 표시되는 경우를 **동차좌표계**라고 함. 동차좌표계의 한 점은 
카메라의 핀 홀로부터 이미지의 한 점을 통과하는 빛줄기. 빛줄기 상의 어떤 점도 카메라 핀 홀로 날아와 초점거리 f만큼 떨어진 상에 맺히면
그 지점의 동차좌표계는 항상 (f * x/z, f * y/z, f).   
![image](https://user-images.githubusercontent.com/69246778/127080560-d4fbae50-e0a3-4fcb-9b78-0a293f0c9e91.png)   
   
주어진 두 카메라에 대해 상대적 회전과 이동거리를 안다면 그림과 같이 매칭된 특징점을 이용해 삼각측량을 할 수 있음. 삼각측량을 통해
해당 지점의 3차원 위치를 알 수 있음. 두 카메라의 상대적 자세 추정은 두 이미지에서 매칭된 특징점 쌍을 이용해 알 수 있음. 
카메라의 내부 파라미터는 이미 구해져 있어야 함.   
![image](https://user-images.githubusercontent.com/69246778/127091579-e876c8fc-3ce2-4424-981f-19783dee76d6.png)   

#### 1.3.2.A. 삼각측량
두 카메라의 상대적 자세를 통해 해당 지점의 3차원 위치를 추정하는 것. 두 카메라의 상대적 위치는 **8-point algoritm**(8개 이상의 
특징점 쌍들이 주어지면 두 카메라의 상대적 회전과 이동정도를 알아 낼 수 있음)이나 **Oepn CV**의 **findEssentialMat() 함수**나 **decomposeEssentialMat() 함수**를 사용해 구할 수 있음.   
각 카메라마다의 내부,외부 파라미터를 알고 있으므로 각 이미지의 특징점들로부터 월드 좌표계에서 각각 빛줄기를 그릴 수 있고 이 두 빛줄기가
만나는 한 지점이 해당 특징점의 3차원 좌표값임.   
![image](https://user-images.githubusercontent.com/69246778/127092218-338d4073-7aa3-4d39-8def-3d73dd61cec1.png)   
   
Open CV의 triangulatePoints()함수로 구현.




##### [HMD](https://terms.naver.com/entry.naver?docId=3586641&cid=59277&categoryId=59278)
Head Mounted Display(=Face Mounted Display), 안경처럼 착용하고 사용하는 모니터.   
![image](https://user-images.githubusercontent.com/69246778/126945688-4e7cd9fb-8667-4994-be55-bac3893eced5.png)

##### [Affine transform](https://wiserloner.tistory.com/849)
영상을 구성하는 픽셀의 배치 구조를 변경함으로써 영상 모양을 변경하는 geometric transform의 일종으로 영상의 평행이동, 확대 및 축소,
회전 등의 조합으로 만들 수 있는 기하학적 변환을 통칭한다. 픽셀의 위치를 일정한 규칙에 따라 옮김으로써 영상 변화를 이루는 것.

##### [투사 변환](https://wordrow.kr/%EC%9D%98%EB%AF%B8/%ED%88%AC%EC%82%AC%20%EB%B3%80%ED%99%98/)
3차원 도형을 2차원 평면 위에 표시하기 위한 투영 변환 중 하나. 

##### [병렬처리]

##### [GPU가속](https://namu.wiki/w/GPGPU?from=GPU%20%EA%B0%80%EC%86%8D)
CPU가 맡았던 연산을 GPU에도 사용해 연산 속도를 향상 시키는 기술이다. 흔히 '하드웨어 가속'이라고 하면 GPU를 가리키는 경우가 많다.

##### [CCD센서](https://ko.wikipedia.org/wiki/%EC%A0%84%ED%95%98%EA%B2%B0%ED%95%A9%EC%86%8C%EC%9E%90)
전하결합소자, 빛을 전하로 변환시켜 화상(畵像)을 얻어내는 센서

##### [Camera obscura]
조그만 구멍으로 빛을 통과시켜 맞은편 벽에 상을 맺게하는 암실
