---
layout: post
title: VisualSLAM
description: |
  [K-mooc 자율주행자동차기술(국민대학교) 강의](http://www.kmooc.kr/courses/course-v1:KMUk+CK-KMUK_02+2021_1/course/)
  
tags:
  - autonomous
use_math : true
comments : true
author: Udayeon
published: true
---

Visual SLAM

* 1. Visual SLAM 기술 개요
  * 1.1 Visual SLAM
  * 1.2 특징 검출과 특징 매칭
  * 1.3 카메라 자세 추정
  * 1.4 특징점 탐색 실습
* 2. VisualSLAM 기술 기초
  * 2.1 초기 카메라 추정
  * 2.2 카메라 트래킹
  * 2.3 이중 시점 분석 실습
* 3. VisualSLAM 기술 심화
  * 3.1 국소 지도 확장과 전역 지도 정합
  * 3.2 루프 결합과 카메라 위치 재조정
  * 3.3 카메라 위치 재조정 실습
 
 
# 1. Visual SLAM 기술 개요
* * *

## 1.1 Visual SLAM
* * *

### 1.1.1 SLAM기술이란?
동시적 위치추적 및 지도 작성기술(Simultaneous localization and mapping)   
![image](https://user-images.githubusercontent.com/69246778/127098829-2cf5f730-02ad-4379-920e-e6432f4444fb.png)   
   
* Mapping : 미지의 영역에 대해 환경지도를 작성.   
* Localization : 작성된 지도상에서 자신의 위치가 어딘지 추정. 자기 위치가 어딘지 알아야 새롭게 탐사한 영역에 대한 정보를 현재 지도에 정확히 붙일 수 있음.   

### 1.1.2. Visual SLAM 이란?
**시각 데이터**를 이용해 동시적 위치추적 및 지도 작성을 하는 알고리즘.

### 1.1.3. Visual SLAM 응용분야

* 자율주행 자동차   
인지 기술로 작동   
EX ) 구글의 웨이모   
지붕에 달린 라이다가 시각 센서로 작동하여 지도 정보가 없는 곳을 지나면 알고리즘으로 지도를 작성(Mapping)하고 이미 지도를 확보한 곳에서는
현재 자동차가 어디에 있는지를 파악함(Localization).   
   
* 증강 현실   
모바일 폰이나 HMD에 달린 카메라가 주변 환경에 대한 시각 데이터를 수집하고 Visual SLAM으로 환경지도와 현재 위치를 얻어낸다. 
그리고 가상 물체를 지도상에 배치해 실제로 있는 것과 같이 표현.   
   
* 로봇청소기   
로봇청소기를 처음 사서 집을 한 번 훑어보게 하면 Visual SLAM이 지도를 작성함. 정확한 지도를 위해 위치 추정도 동시에 진행.   
      
#### 1.1.3.A 로봇청소기로 알 수 있는 자율주행의 문제   
* Kidnapping   
전원이 꺼진 로봇청소기를 사람이 직접 들어 다른 방에 옮겨 놓고 전원을 켜는 경우.   

* 위치 재조정   
Kidnapping된 로봇이 전원이 꺼지기 전 마지막으로 알고 있던 위치는 잘못된 정보라는 것을 인지하고 지도에서 자신이 자리잡고 있는 새 위치를 빠르게 인식하는 것.   
   
* 업데이트    
자신이 기존에 알고 있던 지도를 현재 상황에 맞에 업데이트하는 것.   
   
**자율주행의 경우** 시동이 꺼지기 전 자동차의 위치와 시동이 다시 켜진 위치가 다르거나 도로의 지형이 바뀌거나 지도에는 없는 큰 
장애물이 나타나는 경우, 자율주행 자동차는 기존의 지도 정보를 업데이트해 가며 안전히 운행해야 함.   

### 1.1.4. Visual SLAM System Component   
* Visual Sensor   
(일반적으로 카메라)이미지 데이터를 수집.   
   
* 특징점 검출과 매칭   
** 특징점 검출** : VisualSLAM System이 영상을 손쉽게 다루기 위해 필요한 영상처리 과정. 한 장을 빠르고 효과적으로 처리하기 위해 
이미지로부터 특징이 되는 지점을 검출하는 과정   
   
** 특징점 매칭** : 서로 다른 두 영상에서 검출한 특징점 사이의 대응관계를 구하는 과정.   
   
   
특징점 검출과 매칭으로 처리된 영상들이 **VisualSLAM System**에서 다음과 같이 활용됨.   
- 카메라 자세추정 : 같은 장면을 서로 다른 시점에서 바라본 이미지로 **두 카메라의 상대적 위치를 추정** 추후, 이미지 두 장으로부터 **삼각
  측량**을 이용해 3차원 위치를 추정할 수 있음 **(3차원 지도 작성)**   
- 카메라 트래킹 : 카메라가 돌아다니면서 이미지를 얻을 때 이미지의 특징점으로 **지도 내 카메라의 위치를 추정**   
- 국소적 지도관리 : 카메라가 돌아다니면서 경계에 다다를 때 지도를 넓히는 것.   
- 전역 지도관리 : 전체 지형을 둘러볼 때 위상적으로 문제가 없도록 연결고리 형태로 지도 구성.


## 1.2. 특징 검출과 매칭
* * *

### 1.2.1. 특징 검출과 매칭이란?
카메라가 찍은 이미지에 대한 특징을 검출하고 서로 다른 이미지에서 검출된 특징간의 매칭을 계산하는 과정으로 SLAM을 돌리기 위한 가장
기본적인 데이터 전처리 과정임.

### 1.2.2. 특징 검출
3차원 장면을 카메라로 찍게되면 너비x높이의 개수만큼 픽셀로 구성된 이미지를 얻게되는데, 이는 **SLAM이 처리하기에 너무 방대함.** 
따라서, SLAM시스템에서 이미지 전체에 대한 데이터를 넘겨주기 보다는 **특징 검출을 통해 이미지를 특징점 위주로 줄인 후** 해당 지점만 
SLAM System에 넘겨줌. 그럼 **SLAM System이 가볍고 빠르게(효율적으로)** 돌아갈 수 있음.   

#### 1.2.2.A. 특징점
그렇다면 어떤 점이 특징점이 될 수 있을까?   
**주변의 다른 지점과 비교했을 때 고유하게 인식될 수 있는 지점**을 특징점이라고 함. 보통 특징점은, 육안으로 봤을 때의 
모서리 지점(corner point)에 해당함.   


#### 1.2.2.B. 특징 검출 알고리즘
**Harris corner detection 알고리즘**   
해리스가 제안한 자가유사도(Self similarity) 기반의 알고리즘.   

1. 다음과 같이 주어진 이미지에 빨간색, 노란색, 녹색으로 표시된 3지점을 살펴봄.   
![image](https://user-images.githubusercontent.com/69246778/126956046-d5156937-f841-447a-8894-ec1034d6dfb3.png)   
   
2. 먼저 빨간색 지점(작은 빨간 박스). 빨간색 지점 중심으로 이미지 조각인 패치를 생성(커다란 빨간박스).   
빨간색 패치 내의 한 픽셀을 새로운 지점(주변 지점)으로 삼고 그 지점에 대한 새로운 패치를 생성(커다란 회색박스).   
이때 빨간색 패치와 회색 패치를 비교해 보면 둘이 같아서 구분이 안됨.   
빨간색 패치 내의 다른 지점들도 이런 방식으로 보면 비슷해서 구분 안됨. 따라서 빨간색 지점은 특징점이 될 수 없음.   
![image](https://user-images.githubusercontent.com/69246778/126956090-5c2dea15-f6e3-4a31-9cc7-e20e8f1be531.png)   
   
3. 노란색 지점을 중심으로 패치를 잡고 위와 같은 방식으로 진행함. 만약 edge방향으로 주변 지점을 잡으면 회색패치와 구별이 안됨.   
노란색도 특징점이 될 수 없음.   
![image](https://user-images.githubusercontent.com/69246778/126956123-348a6c47-3473-4639-b950-ba3dfb952e29.png)   

4. 녹색 지점을 중심으로 패치를 잡고 다시 반복. 어떤 지점을 잡아도 주변 지점과 확실하게 구별됨. **특징점**   
![image](https://user-images.githubusercontent.com/69246778/126956171-c0e65828-c76f-47c8-bd24-f091f2275272.png)   
   
**FAST corner detector**, **ORB corner detector**등 다양한 알고리즘이 있음.

### 1.2.3. 특징매칭
두 이미지에서 따로 뽑은 특징점이 실제로 같은 지점이라고 생각된다면 서로 짝을 지어줌. **두 이미지 사이의 관계 계산**에 사용.   
이미지에 찍힌 내용이 다른 이미지에도 들어 있어 **두 영상이 유사한지 여부**를 측정하거나, **카메라 자세 추정**에 활용.
특징묘사와 특징매칭 단계로 구성된다.   

#### 1.2.3.A. 특징묘사  
**특징점을 비교 가능한 자료 형태(특징 묘사자)로 바꿔줌.** 동일한 지점은 같은 특징묘사자로, 다른 지점은 확연히 다른 특징 묘사자로 
구별할 수 있게 하는 것이 **쫗은 특징 묘사자**임. 어떤 지점이 확대되거나 축소되거나 회전되거나 어파인 또는 투사변환 된다든지 조명 조건이 
살짝 달라진 환경에서도 동일 지점은 거의 비슷한 특징 묘사자를 만들어내야 함. **특징점 묘사 결과를 활용하여 유사도가 높은 쌍끼리 매칭**   
   
**특징 묘사자 종류**   
* **SIFT**(Scale- Invariant Feature Transform)   
David Lowe가 제안한 대표적인 특징 묘사자. 이미지의 크기와 회전에 불변하는 특징을 추출.   
1. 특징점 주변에 밝기 변화 크기에 대한 중심축 잡기. 밝기 변화가 큰 방향으로 축을 잡음.   
![image](https://user-images.githubusercontent.com/69246778/127101765-5c0086fb-48e6-4ddd-8967-a3bb8b2e524b.png)   
      
2. 축과 같은 방향으로 패치를 잡고 패치 내 각 지점마다 밝기 변화량이 가장 크게 일어나는 Gradient방향 수집.   
![image](https://user-images.githubusercontent.com/69246778/127101847-609fd037-52cf-49da-8457-a2ef1deeb8fe.png)   
   
3. 수집한 Gradient를 부분별로 8개의 주 방향에 대해 얼마나 많이 누적되어 있는지에 대한 정보를 수집. 이 누적 결과가 특징 묘사자로 쓰임.   
![image](https://user-images.githubusercontent.com/69246778/127101931-61e29d8d-b4fb-460e-82c9-3e811a670ce6.png)   
   
* **HoG** : 각 지역별 Gradient를 고려   
      
* **BRIEF** : 임의의 패턴을 이용해서 고른 두 주변 지점에 대해, 상대적으로 어둡고 밝은 패턴을 기록   
   
#### 1.2.3.B. 특징매칭   
특징 묘사자의 유사도를 정의하고 서로 다른 이미지에서 등장한 **유사한 특징끼리 서로 짝을 지어주는 과정**,
**유사도 측정**은 특징 묘사자 벡터 거리 측정 방식이 주를 이룸. 매칭으로 고려할 쌍이 너무 많으므로 병렬처리나 GPU를 이용한 가속을 고려.   
   
**잘못된 매칭을 거르는 방법**   
* **Cycle consistency**: A이미지에서 뽑힌 특징점을 기준으로 B에서 유사한 특징점을 찾는 방식과, B에서 뽑힌 특징점을 기준으로 A에서의 
유사한 특징점을 찾는 방식이 다른 경우 Cycle consistency가 깨진 것.   

* **Epipolar consistenccy**: 삼각측량을 사용할 때 삼각형의 세 점을 항상 같은 평면에 놓여야 한다는 점을 이용해 잘못된 매칭 쌍을 걸러냄.   

## 1.3. 카메라 자세 추정
* * *

### 1.3.1. 카메라 기하 구조 - 단일 시점 기하
카메라 한 대가 정의하는 기하구조, 한 점이 3차원 공간에서 빛줄기로 해석됨. 카메라는 렌즈, 바디, 필름 또는 CCD센서로 구성됨. 
Camera obscura로부터 만들어짐 따라서 가장 단순한 카메라의 구조는 빛이 통과하는 **핀 홀**과 상이 맺히는 **이미지**만으로 구성된다고 가정. 
이 때 다음과 같이 f:거리이미지 = d:실제이미지 
![image](https://user-images.githubusercontent.com/69246778/127080037-bc0947b3-2bb9-4742-bf3e-574a13225652.png)   
   
그런데 상이 뒤집혀 맺히므로 좌표계의 부호를 엄밀히 고려해야 하는 계산상의 불편함이 있으므로 다음과 같이 이미지가 **카메라의 앞**에 
위치한다고 가정할 수 있음.    
![image](https://user-images.githubusercontent.com/69246778/127080091-d22bc88d-0529-4816-a2ac-64f3d7083c56.png)   
      
이미지만으로 초점거리를 바로 알아내기는 쉽지 않음. 이를 추정하는 걸 **카메라 내부 파라미터 추정**이라 함.   
![image](https://user-images.githubusercontent.com/69246778/127081369-633cc254-6f0c-4a4a-8b06-ffd41b0a5066.png)   
   
 
### 1.3.1.A. 카메라 내부 파라미터  
주점(Principle Point)의 위치, 초점거리 등을 말함.   
주어진 정보가 이미지 뿐일 때 이를 분석해 카메라 내부 설정을 추정하는 것을 **카메라 내부 파라미터 추정**이라한다. SLAM의 지도 작성과
위치추정에 필요하다.

* **이미지의 주점, 초점거리를 알 수 없는 경우**   
일반적으로 **주점**은 이미지의 중심인데 이미지가 **크롭해서 저장**되면 주점이 이미지의 중심이 아닐수 있음. **인터넷에서 다운받은 
이미지**는 크롭됐는지 여부를 알 수 없으므로 주점이 이미지의 중심인지 확정할 수 없음.   
**초점거리**를 알 수 없는 경우도 많음. 초점거리가 특징되지 않으면 특징점의 위치를 알아도 핀홀과 특징점을 잇는 빛줄기를 파악할 수 없음. 
즉, 같은 이미지를 얻어도 서로 다른 초점거리로 찍혔다면 3차원 공간에서 **서로 다른 빛줄기로 상이 맺했다고 생각할 수 있음.**   
   
- **Calibration** 
카메라 내부 파라미터 추정 기능 지원   
  - openCV : calibrateCamera() 함수   
  - MATLAB : calibration tool box   
      
* **카메라 내부 행렬**   
카메라 내부 파라미터를 수학적으로 표현한 행렬.    
![image](https://user-images.githubusercontent.com/69246778/127083128-242ffe31-59fb-4010-ab88-2e510fe967d4.png)   
**f_x, f_y** : 초점거리   
**C_x, C_y** : 주점의 위치   
   
이렇게 알아낸 내부 행렬을 이용해 **이미지 상의 한 점**을 **카메라 좌표계 기준의 3차원 빛줄기**로 변환.   

### 1.3.1.B. 카메라 외부 파라미터
SLAM이 만들어 낸 3차원 환경지도는, 카메라가 지도작성 좌표계를 기준으로 어떻게 회전하고 이동하였는지에 대한 
정보(카메라가 세상에 놓인 자세). 카메라의 **회전**과 **이동 정보**를 합친 행렬 rt형태를 띰.   
   
 카메라가 세상에 놓인 자세를 추정했다고 가정하면 카메라의 외부 행렬을 이용해 카메라 좌표계로 표현된 정보를 월드 좌표계로 변환할 수 있음.
![image](https://user-images.githubusercontent.com/69246778/127083683-ee9f3736-858e-4d0c-b241-a3a52f347433.png)   
   
|카메라 내부 파라미터| 카메라 외부 파라미터|
|:-----------------|:-------------------|
|이미지의 한점->카메라 좌표계로 표현된 빛줄기|카메라 좌표계로 표현된 정보 -> 월드 좌표계|

**즉, 이미지의 한 점이 월드 좌표계로 변화**


- **Calibration**
카메라 외부 파라미터 추정 기능 지원   
  - openCV : calibrateCamera() 함수    
  - MATLAB : calibration tool box   
   
* **카메라 외부 행렬**
카메라 외부 파라미터를 회전과 이동이 결합된 형태로 표현    
![image](https://user-images.githubusercontent.com/69246778/127083771-895f5c19-edd5-49f9-89bb-e36b8137d51c.png)
월드 좌표계가 빨간색, 녹색, 파란색 축으로 표현된 것은 현재 카메라의 회전과 위치를 정확히 추정한 결과임.

   
   
   
   
   
   
   
### 1.3.2. 카메라 기하 구조 - 이중 시점 기하
카메라 두 대가 정의하는 기하구조, 사람의 두 눈과 같이 삼각측량을 통해 두 장의 이미지로부터 3차원을 인식.
단일 시점 기하와 거의 같지만, **핀 홀 중심으로 카메라의 3차원 좌표계**를 고려함.   
![image](https://user-images.githubusercontent.com/69246778/127080522-830048ce-d4fb-4894-81aa-6432cbdcb96c.png)   
   
상이 카메라 앞에 맺힌다고 가정. 물체 위의 한 지점을 카메라 좌표계(x,y,z)로 표현. 해당 지점이 초점거리 f에 위치한 상에 맺힌다면
상에 맺힌 지점의 좌표는 (f * x/z, f * y/z, f). 즉, 이미지가 초점거리 f만큼 떨어져 있다면 이미지상의 한 점은 **3차원 좌표계**로 표시됨.
이처럼 **이미지의 한 점이 2차원 좌표계가 아닌 3차원 좌표계로 표시**되는 경우를 **동차좌표계**라고 함.
빛줄기 상의 어떤 점이든 카메라 핀홀로 날아와 초점거리 f만큼 떨어진 상에 맺히면 그 지점의 동차좌표계는 항상 (f * x/z, f * y/z, f).   
![image](https://user-images.githubusercontent.com/69246778/127109706-ca2ea2af-753a-499e-b1d2-0d63d9cc984f.png)   
   
주어진 두 카메라에 대해 상대적 회전과 이동거리를 안다면 그림과 같이 매칭된 특징점을 이용해 삼각측량을 할 수 있음. 삼각측량을 통해
해당 지점의 3차원 위치를 알 수 있음. 두 카메라의 상대적 자세 추정은 두 이미지에서 매칭된 특징점 쌍을 이용해 알 수 있음. 
카메라의 내부 파라미터는 이미 구해져 있어야 함.   
![image](https://user-images.githubusercontent.com/69246778/127091579-e876c8fc-3ce2-4424-981f-19783dee76d6.png)   
   
#### 1.3.2.A. 삼각측량
두 카메라의 상대적 자세를 통해 해당 지점의 3차원 위치를 추정하는 것. 두 카메라의 상대적 위치는 **8-point algoritm**(8개 이상의 
특징점 쌍들이 주어지면 두 카메라의 상대적 회전과 이동정도를 알아 낼 수 있음)이나 **Oepn CV**의 **findEssentialMat() 함수**나 
**decomposeEssentialMat() 함수**를 사용해 구할 수 있음.   
**각 카메라마다의 내부,외부 파라미터를 알고 있으므로 각 이미지의 특징점들로부터 월드 좌표계에서 각각 빛줄기를 그릴 수 있고 이 
두 빛줄기가 만나는 한 지점이 해당 특징점의 3차원 좌표값임.**   
   
![image](https://user-images.githubusercontent.com/69246778/127092218-338d4073-7aa3-4d39-8def-3d73dd61cec1.png)   
   
Open CV의 triangulatePoints()함수로 구현.



## 1.4. 특징점 탐색 실습
* * *









# 2. Visual SLAM 기술 기초
* * *

## 2.1. 초기 카메라 추정
* * *

### 2.1.1. 초기 지도(initial Map)작성
초기 지도부터 서서히 확장해 나가면서 전체 지도를 구성하게 됨. 기 지도에서 결정된 단위 거리가 전체 지도의 단위 거리를 결정하고 
초기 지도 작성시 발생한 에러가 누적되어 전체 지도에 퍼질 수도 있기 때문에 강건하고 정밀하게 작성하는 것이 중요.   

#### 2.1.1.A. 강건한 초기 지도를 위해 필요한 것들
**강건한 특징점 매칭 + 강건한 카메라 자세 추정 + 강건한 삼각측량 기법**   

#### 2.1.1.B. 매칭된 특징점에 대한 삼각측량   
3차원 장면에 있는 한 점 P_i는 이를 서로 다른 각도에서 찍은 두 카메라에 담기게 됨. 만일 두 카메라의 상대적 자세와 두 카메라 이미지들 간의
특징점 매칭 결과를 알고 있다하면, 두 카메라의 중심을 잇는 직선과 각 카메라에서 점 P_i로 향하는 각도를 알고 있는 셈. 따라서 삼각형의 원리로
**P_i의 3차원 위치를 측정할 수 있게 됨.**   
![image](https://user-images.githubusercontent.com/69246778/127244011-ebacd92f-83c0-4d08-ad3f-268c0a1caa7c.png)   

### 2.1.2 양안 카메라
삼각 측량의 기본 원리를 활용한 장비. 사람의 두 눈처럼 카메라를 특정 거리만큼 떨어뜨려 놓고 설치하는 방식으로 카메라의 상대적 자세를 미리 
알 수 있도록 장치를 구성하는 것.

#### 2.1.2.A. 양안 카메라를 이용한 삼각측량
삼각 측량을 위해 필요한 정보 중 카메라의 상대적 자세는 이미 정확히 알고 있으므로 **두 이미지의 특징 매칭**으로부터 비교적 정확한 삼각 측량
결과를 얻을 수 있음. 

### 2.1.3. 강건한 삼각측량을 위한 필요 요소

#### 2.1.3.A. 충분한 너비 기준선 확보가 중요
![image](https://user-images.githubusercontent.com/69246778/127244320-8efeaea8-09e8-4e36-b1f8-d5926d58b24c.png)  
기준선이 좁으면 특징 매칭에서 발생한 조그마한 오차도 매우 큰 범위로 증폭됨. 충분히 멀면 특징 매칭에서 생긴 조그마한 오차는 무시할 
만큼의 범위로 좁아짐. 야간 카메라시에 기준선을 넓게 하면 좋음. 근데 넘 극단적으로 넓으면 두 이미지의 공통 부분이 줄어들어 특징 매칭이
잘 안돼서 오히려 강건한 삼각측량 X. 해결하려는 문제에 따라 정닥한 기준선 선택.


#### 2.1.3.B. 강건한 특징 매칭 획득
이미지의 특징 검출 결과에 대한 **유사성**에만 기반. 잘못된 결과가 나올 수도 있음. 잘못된 결과 거르는 방식
* **Cycle consistency(순환일관성)**  
특징 매칭 된 두 이미지의 특징점에 대해 서로 가장 최선의 선택으로 뽑은 결과였는지를 묻는 것. 이미지1 -> 이미지2 로 선택한 특징점과
이미지2 -> 이미지1로 선택한 특징점이 서로 같은지 묻는 것. 같으면, 서로를 원해서 선택한 것이므로 강건한 매칭이 됨.    
   
* **Epipolar**   
특징 매칭된 3차원 지점과 두 카메라의 3차원 위치는 삼각형을 이뤄야 하므로 한 평면에 놓여야 한다는 제약조건.   
   
* **Homography**
서로 다른 각도에서 동일한 평면을 찍었다는 **3차원 장면의 사전정보**가 있다면 homography변환을 만족하지 않는 특징 매칭결과를 제거.
![image](https://user-images.githubusercontent.com/69246778/127245335-c8a68f66-452b-461c-95e9-6c4b0ef9254a.png)

#### 2.1.3.C. 특징 추적
양안 카메라는 두 카메라를 기계적으로 구상한 것이므로 두 카메라의 상대적 자세를 알 필요가 없기 때문에 특징 추적 필요 없음.
한 대의 카메라로 양안 카메라를 흉내 내어 **Monocular Visual SLAM(단안 카메라 기반 슬램)** 구현.
카메라가 시간이 지남에 따라 움직인다고 가정하고 서로 다른 시간에서 획득한 두 이미지를 마치 양안 카메라가 찍은 두 이미지로 처리하는 방식.
시공간적으로 너무 가까운 이미지는 계산하지 않는 것이 좋음. 이처럼 적당히 멀리 떨어져있는 이미지를 **Keyframe**이라고 함. 

## 2.2. 카메라 트래킹
* * *

## 2.3. 이중 시전 분석 실습
* * *










##### [FAST corner detector]
##### [ORB corner detector]


##### [HMD](https://terms.naver.com/entry.naver?docId=3586641&cid=59277&categoryId=59278)
Head Mounted Display(=Face Mounted Display), 안경처럼 착용하고 사용하는 모니터.   
![image](https://user-images.githubusercontent.com/69246778/126945688-4e7cd9fb-8667-4994-be55-bac3893eced5.png)

##### [Affine transform](https://wiserloner.tistory.com/849)
영상을 구성하는 픽셀의 배치 구조를 변경함으로써 영상 모양을 변경하는 geometric transform의 일종으로 영상의 평행이동, 확대 및 축소,
회전 등의 조합으로 만들 수 있는 기하학적 변환을 통칭한다. 픽셀의 위치를 일정한 규칙에 따라 옮김으로써 영상 변화를 이루는 것.

##### [투사 변환](https://wordrow.kr/%EC%9D%98%EB%AF%B8/%ED%88%AC%EC%82%AC%20%EB%B3%80%ED%99%98/)
3차원 도형을 2차원 평면 위에 표시하기 위한 투영 변환 중 하나. 

##### [병렬처리]

##### [GPU가속](https://namu.wiki/w/GPGPU?from=GPU%20%EA%B0%80%EC%86%8D)
CPU가 맡았던 연산을 GPU에도 사용해 연산 속도를 향상 시키는 기술이다. 흔히 '하드웨어 가속'이라고 하면 GPU를 가리키는 경우가 많다.

##### [CCD센서](https://ko.wikipedia.org/wiki/%EC%A0%84%ED%95%98%EA%B2%B0%ED%95%A9%EC%86%8C%EC%9E%90)
전하결합소자, 빛을 전하로 변환시켜 화상(畵像)을 얻어내는 센서

##### [Camera obscura]
조그만 구멍으로 빛을 통과시켜 맞은편 벽에 상을 맺게하는 암실
