---
layout: post
title: Semminar2 - SLAM paper 
description: |
  
tags:
  - autonomous
use_math : true
comments : true
author: Udayeon

published: true
---

# (Review) LeGO-LOAM : Lightweight and Ground-Optimized Lidar Odometry and Mapping on Variable Terrain

[LeGO-LOAM : Lightweight and Ground-Optimized Lidar Odometry and Mapping on Variable Terrain](https://ieeexplore.ieee.org/document/7995716/metrics#metrics)   
- **Author** Tixiao Shan and Brendan Englot
- 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems(IROS),Madrid,Spain,October 1-5,2018
{:.message}

**논문 선정 기준**   
* 기선정한 Lidar SLAM논문이 개념 설명 위주의 리뷰논문이라 그 외 추가적인 기술이 없는지 궁금했음.
* 본 논문은 기선정한 논문을 Reference로 삼아 더 효과적인 방안을 Proposed함.
* 구글 학술검색 기준 인용횟수 278


  - [Abstract]
  - [1. Introduction]
  - [2. System Hardware]
  - [3. Lightweight Lidar Odometry And Mapping]
  - [4. Experiments]
  - [5. Conclusion And Discussion]

# Abstract
* * *

# Introduction
* * *

# System Hardware
* * *



# 3. Lightweight Lidar Odometry And Mapping
* * *

## 3.A. System Overview
proposed된 framework의 overview는 **Fig1**과 같음.   
![image](https://user-images.githubusercontent.com/69246778/128106782-27a2ff70-a9f0-4405-a118-e00f4711892a.png)   
Lidar의 Point Cloud로 3D정보를 input으로 받고, 6DOF의 Pose estimation을 output으로 출력함. 전체 System은 5개의 module로 나뉨.

|:---------------|:-----------------------------------------------------------------|
|**Segmentation**| Single scan의 point cloud를 가져와서 Range image위에 투영하고 분할함|
|**Feature extraction**|분할된 Point cloud에서 Feature extraction|
|**Lidar odometry**|Consecutive scan과 관련된 변환을 찾기 위해 이전 모듈로부터 추출한 feature를 사용함|
|**Lidar mapping**| featrue들을 global point cloud map에 registration|
|**Transform integration**|Lidar odometry와 lidar mapping의 pose추정 결과를 통합하고 최종 pose 추정을 출력|


proposed된 system은 기존 일반화 된 LOAM의 framework와 관련하여 지상차량(Ground vehicle)의 효율성과 정확성을 추구함.

## 3.B. Segmentation
![image](https://user-images.githubusercontent.com/69246778/128108523-67908690-d000-4922-917e-2cbe651f33cd.png)
시각 t에서 얻은 Point cloud를 Pt라 할 때, 그 안의 각 Point들이 pi(p1,p2,p3 ...).Pt는 range image에 먼저 투영.
이 때, 사용하는 하드웨어인 VLP-16은 수평해상도 0.2◦, 수직해상도 2◦ 이므로 range image의 해상도는 1800 X 16
![image](https://user-images.githubusercontent.com/69246778/128109494-d22437aa-7880-4571-b0de-47fbc38071b3.png)

Pt내의 유효한 pi는 range image에서 unique pixel로 표시됨. range value ri는 pi와 관련있는 값으로 pi부터 센서까지의 유클라디안 거리.
일반적으로 땅은 경사져있고 평평하지 않음. ground point를 추출할 필요가 있으므로 segmentaion전에 range image의 column-wise 평가를
수행해 ground plane을 추정함. 그리고나서, ground를 나타낼 수 있는 point는 ground point로 분류되어 segmentation에 사용하지 않음.   
   
그런 다음, image-based segmentation method가 range image에 적용되어 group point를 여러 cluster로 분류함. 같은 cluster의 point에는
같은 label이 할당됨. 기억해야할 건, 앞에서 따로 분류했던 ground point는 특수한 유형의 cluster임. point cloud에 segmentation을
적용하는 것은, processing 효율과 featrue extraction의 정확도를 향상시킴. 로봇이 noisy한 환경에서 작동한다고 할 때를 생각해보면,
나뭇잎 같이 작은 사물들은 두 번 scan한다고 두 번 다 연속적으로 관찰되지 않으므로 사소하고 신뢰할 수 없는 featrue임. 따라서, segment된
point cloud에서, cluster내의 점이 30개 미만인 cluster를 생략하면 빠르고 신뢰할 수 있는 feature extraction이 가능함.
다음 그림이 segmentation 전후를 나타낸거임.
![image](https://user-images.githubusercontent.com/69246778/128112664-f2a39b9a-e109-4c14-8c8b-59095ad25b90.png)
   
**(Fig2-a)** original에는 주변 초목에서 얻은 신뢰할 수 없는 Point들을 포함하고 있음    
**(Fig2-b)** ground point는 빨간색으로 표현하고, segmentation한 이후의 point cloud라서 큰 사물들의 point만 남음. 그리고 이 점들만 
range image에 저장됨.   
   
이러한 점들로부터 3가지 특성을 얻을 수 있음   
(1) ground point 및 segmented point   
(2) range image의 column,row index   
(3) range value   
   
## 3.C. Feature Extraction
Feature extraction 과정은 [20]과 유사하지만, raw point cloud가 아닌 ground point와 segmented point에서 feature를 extraction한다는 
점이 다름. S를 range image의 같은 행에 있는 연속적인 point pi의 집합이라고 해보자. 
   
(이해 ㄴㄴ)S에 속한 point의 절반은 pi의 양쪽에 있음.(?)   
본 논문에서 |S|를 10으로 설정(S에 속한 point수? 먼 의미?)   
   
segmentation동안 계산된 range value를이용해 S에 속한 pi의 roughness를 평가할 수 있음.
![image](https://user-images.githubusercontent.com/69246778/128118162-80a55819-bff8-418d-a18c-7b33c1e78542.png)

* 각 점들간의 거리가 멀면 roughness가 크고
* S가 작다는 건 같은 row에 속한 point가 적다는 거니까 연속성이 떨어짐 --> roughness 증가?
* r이 작다는 건 각 pi부터 센서까지의 거리가 멀다는 건데 그러면 왜 roughness가 증가? 먼상관..
   
모든 방향으로부터 feature를 균등하게 extraction하기 위해, range image를 horizontal하게 나눠서 여러 개의 균등한 sub-image로 만듦.
그런 다음, 각 행의 point를 roughness를 평가하는 c값에 따라서 분류함. LOAM과 비슷하게 threshold c_th를 사용해서 feature의 
다양한 type을 분류함. 만약에 c값이 c_th보다 크면 edge point라하고, c값이 c_th보다 작으면 planar point임. 

* c가 크다는건 rough한거니까 edge, 작으면 smooth한거니까 planar

그 다음 할일임. 일단 6개의 sub-image로 나누니까 각 sub image resolutio은 300x16임. 
1. sub image의 각 row에서 최대c, 최소c를 이용해 edge 및 planar feature point를 선택   
![image](https://user-images.githubusercontent.com/69246778/128120149-3c2f058f-e2ac-4250-8c77-04d205fbc967.png)   
: ground에 속하지 않는, 최대 c를 갖는 edge feature point   
   
![image](https://user-images.githubusercontent.com/69246778/128120190-29392283-c671-4640-880b-c36e116cb134.png)   
: ground point가 되는, 최소 c를 갖는 planar feature point   
   
2. 위에서 선택한 feature (point?)의 집합   
![image](https://user-images.githubusercontent.com/69246778/128120516-3d0c78a7-9bea-4e9f-bb8e-9952bf2f8c48.png)   
: sub image의 모든 edge 및 planar feature    
![image](https://user-images.githubusercontent.com/69246778/128121201-4da2240c-263b-4bb6-bfc8-0dad842ed681.png)
   
3. sub image의 각 row에서 최대c, 최소c를 이용해 edge 및 planar feature를 선택.   
![image](https://user-images.githubusercontent.com/69246778/128120755-34f769f2-1c4f-456d-a405-a4a8cbf7116d.png)   
: ground에 속하지 않는, 최대 c를 갖는 edge feature  
   
![image](https://user-images.githubusercontent.com/69246778/128120772-6bb8b9b0-0e37-4f42-bae9-a42b776c7bba.png)   
: ground point가 되는, 최소 c를 갖는 planar feature

4. 이 과정을 거쳐 얻은 모든 edge 및 planar feature
![image](https://user-images.githubusercontent.com/69246778/128120995-87c00bb8-322b-4060-906a-bbda7fe7d456.png)   
![image](https://user-images.githubusercontent.com/69246778/128121226-76fc07f2-0f84-4171-a46b-d68d0753e20f.png)   
![image](https://user-images.githubusercontent.com/69246778/128121310-579e8655-6618-4200-8af6-d983c9b2f6e7.png)   

* 사실상 ground가 가장 rough하니까(?) c가 최대일텐데 이는 제외하고 다루기로 했으므로 이걸 제외했을 때의 최대 c를 고려.
* 최소 c는 왜 ground나 segmented point여도 ㄱㅊ? 차이가 머임?

![image](https://user-images.githubusercontent.com/69246778/128121753-e4335e89-4aea-484c-8cda-a358c36cf3fc.png)
먼솔?

## 3.D. Lidar Odometry
Lidar Odometry는 두 번의 연속적인 스캔 사이에서 센서의 움직임을 추정함. point-to-edge와 point-to-plane의 scan-matching을
사용해 두 scan간의 변환을 수행함. 즉, 시각 t-1일 때의 스캔 feature set로부터 시각t일 때의 feature point에 해당하는 feature를 찾아야 함.
이러한 대응 관계를 찾는 자세한 절차는 [20]에서 보면되고, 우리는 이 feature matching의 정확도와 효율을 향상 시키기 위한 약간의 변수를 소개함.

### 3.D.1. Label Matching
segmentation이후에 시각 t에서의 feature는 각각의 label로 encoding되기 때문에, 우리는 시각 t-1의 feature point집합에서 이와 대응되는 
label을 찾기만 하면됨. 먼저, 시각 t에서의 Planar feature에 있어서, 시각 t-1에서 ground points로 label되어있는 point들만이 
대응하는 planar patch를 찾는데에 사용됨. 시각 t에서의 Edge feature는, 그것에 대응하는 edge line이 segment된 cluster로 얻은 시각t-1의
feature set에 있음. 이런 방식으로 대응관계를 찾는 것은 matching의 정확도를 높여줌. 즉, 두 스캔 사이에서 동일한 개체에 대한 matcing 
일치가 더 많이 발견됨. 이 절차는 잠재적인 correspondence의 후보들을 좁혀줌.

### 3.D.2. Two-step L-M optimization
[20]에서는, 현재 scan으로부터 구한 edge 및 planar

